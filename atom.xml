<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>The Wrong Language</title>
  <subtitle>My random thoughts, often about programming and computers</subtitle>
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="https://sazzer.github.io/blog/"/>
  <updated>2016-09-03T13:23:59.656Z</updated>
  <id>https://sazzer.github.io/blog/</id>
  
  <author>
    <name>Graham Cox</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>OpenID Connect Response Types</title>
    <link href="https://sazzer.github.io/blog/2016/09/03/OpenID-Connect-Response-Types/"/>
    <id>https://sazzer.github.io/blog/2016/09/03/OpenID-Connect-Response-Types/</id>
    <published>2016-09-03T12:40:28.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://openid.net/connect/" target="_blank" rel="external">OpenID Connect</a> Specification extends OAuth2 in a number of ways, one of which is to define some new Response Types that can be used. Unfortunately it doesn’t do the best job in explaining what these all mean. It actually is explained, if you know <a href="https://openid.net/specs/oauth-v2-multiple-response-types-1_0.html" target="_blank" rel="external">where to look</a>, but even then it’s not the clearest document in the world.</p>
<p>Part of the complexity is that the specification allows for combinations of response types, but doesn’t make this as obvious as it could. Instead it makes it look like the combinations are actually whole new response types in their own right, which isn’t quite right.</p>
<p>The actual set of response types are:</p>
<ul>
<li>code - The requester would like an Authorization Code to be returned to them</li>
<li>token - The requester would like an Access Token to be returned to them</li>
<li>id_token - The requester would like an ID Token to be returned to them</li>
<li>none - The requester doesn’t want any of the above to be returned to them</li>
</ul>
<p>The “none” response type is a special case in that it can not be combined with any of the others. The other three can be combined in any way that you want, and you will be requesting all of the details for the combination that you specify. For example:</p>
<ul>
<li>code token - The requester would like both an Authorization Code and an Access Token to be returned to them</li>
<li>token id_token - The requester would like both an Access Token and an ID Token to be returned to them</li>
<li>code token id_token - The requester would like an Authorization Code, an Access Token and an ID Token to be returned to them</li>
</ul>
<p>The further confusion is in the fact that the specification will allow these to be combined in any order, but you will almost always see them in the same order in the various documents. The requests for “code token id_token”, “code id_token token” and “token id_token code” are all actually the same, and should be treated as such.</p>
<p>So why would you want to combine these? The obvious reason is because of the <a href="https://openid.net/specs/openid-connect-core-1_0.html#ImplicitFlowAuth" target="_blank" rel="external">OpenID Connect Implicit Flow</a> and <a href="https://openid.net/specs/openid-connect-core-1_0.html#HybridFlowAuth" target="_blank" rel="external">OpenID Connect Hybrid Flow</a> requiring it. Implicit allows for the client to specify either “id_token” or “id_token token”, and Hybrid allows for the client to specify either “code id_token”, “code token”, or “code id_token token”.</p>
<p>The uses of “id_token token” and “code id_token” are fairly straightforward - they allow for the client to get the ID Token directly and also get an Access Token - either directly or indirectly - to further make API calls to the server. The use of “code token” and “code id_token token” is slightly less obvious - why would you want to get the Access Token AND an Authorization Code, when the only use of an Authorization Code is to swap it for an Access Token? The reason is that an Authorization Code actually does get you more than an Access Token. It also gets you a Refresh Token, which gives you much longer term access to the resources on the server than the Access Token on it’s own likely will.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The &lt;a href=&quot;https://openid.net/connect/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;OpenID Connect&lt;/a&gt; Specification extends OAuth2 in a number of w
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="oauth2" scheme="https://sazzer.github.io/blog/tags/oauth2/"/>
    
      <category term="openid" scheme="https://sazzer.github.io/blog/tags/openid/"/>
    
      <category term="oidc" scheme="https://sazzer.github.io/blog/tags/oidc/"/>
    
      <category term="openid connect" scheme="https://sazzer.github.io/blog/tags/openid-connect/"/>
    
  </entry>
  
  <entry>
    <title>TDD with Static Typed Languages</title>
    <link href="https://sazzer.github.io/blog/2016/05/29/TDD-with-Static-Typed-Languages/"/>
    <id>https://sazzer.github.io/blog/2016/05/29/TDD-with-Static-Typed-Languages/</id>
    <published>2016-05-29T14:12:50.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>TDD, or Test Driven Development, is fairly well acknowledged to be a great idea. The general theory behind it is that you write your tests first, which will initially fail, and then you write just enough code to make the tests pass. Once the tests pass, the code is finished and you can move on to the next bit of functionality. There have been studies done that show that working this way drastically reduces the number of issues in the overall product.</p>
<p>And for some languages, this is all fantastic. You write the tests, the tests fail, you move on to the actual development.</p>
<p>And for yet other languages, this just doesn’t work at all. Specifically I’m thinking compiled, statically typed languages here. The problem with this scenario is, when you write your tests against non-existent types then you break the compilation of the test suite, which in turn breaks <em>every</em> test in the system. And that in turn means that until you’ve implemented at least enough for all of the tests to compile you can’t be sure that you’ve not broken anything else.</p>
<a id="more"></a>
<p>Historically, I have to confess that this has led me down the path of thinking that TDD is just a non-starter in these situations. And so I’ve always carried on working in the same way, writing the code first and the tests after. And, of course, sometimes I then miss some tests out or just don’t get around to them. And as such the whole test suite suffers. Not good.</p>
<p>I’ve just recently come up with a thought to reconcile all of this though. Namely, TDD doesn’t have to apply to all levels of testing. As such, my new plan is to apply the TDD principles to Integration and E2E testing only. Unit Testing I will continue to do after the code being tested has been written, and that’s fine.</p>
<p>Why this line in the sand? E2E testing is literally testing the application as a whole. You write the tests as if a user was using the application, driving the user interface and asserting only on things that are visible to the user. Integration testing is testing at the code level, but it is testing an entire set of work in one go. As such, that entire set should have a single concrete API, and you should test against that API. You are therefore able to define the API first, and write tests against this API before you implement it. The end result here is that everything that you are testing in this way is done so from the outside in, and generally with no access to the innards that you’ve not yet written. This is important, because it means that the entire test suite can be written without any implementation code existing yet, which is exactly the idea behind TDD.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TDD, or Test Driven Development, is fairly well acknowledged to be a great idea. The general theory behind it is that you write your tests first, which will initially fail, and then you write just enough code to make the tests pass. Once the tests pass, the code is finished and you can move on to the next bit of functionality. There have been studies done that show that working this way drastically reduces the number of issues in the overall product.&lt;/p&gt;
&lt;p&gt;And for some languages, this is all fantastic. You write the tests, the tests fail, you move on to the actual development.&lt;/p&gt;
&lt;p&gt;And for yet other languages, this just doesn’t work at all. Specifically I’m thinking compiled, statically typed languages here. The problem with this scenario is, when you write your tests against non-existent types then you break the compilation of the test suite, which in turn breaks &lt;em&gt;every&lt;/em&gt; test in the system. And that in turn means that until you’ve implemented at least enough for all of the tests to compile you can’t be sure that you’ve not broken anything else.&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="testing" scheme="https://sazzer.github.io/blog/tags/testing/"/>
    
      <category term="TDD" scheme="https://sazzer.github.io/blog/tags/TDD/"/>
    
  </entry>
  
  <entry>
    <title>I disagree with HATEOAS</title>
    <link href="https://sazzer.github.io/blog/2016/05/08/I-disagree-with-HATEOAS/"/>
    <id>https://sazzer.github.io/blog/2016/05/08/I-disagree-with-HATEOAS/</id>
    <published>2016-05-08T10:22:01.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>HATEOAS, for anyone who’s not come across it yet, is an additional constraint added on top of normal REST services whereby the only way that clients ever interact with the server is through Hypermedia that the server provides. The server will provide a single starting point, and every single interaction with the server after that is made only through URLs that the server has previously provided to the client. The advantage here is that the server can change independently of the client, and as long as the client only ever follows the URLs provided to it then everything will continue to work correctly.</p>
<p>That all sounds fantastic on the face of it. Completely decoupled evolution of server and client sounds almost idyllic.</p>
<p>Of course, in reality this doesn’t actually work. There are a number of problems with the idea, ranging from the very simple - clients cut corners and just hard-code URLs to specific resources instead of following links - to the very complicated, where changes to the server need to be made that just aren’t backwards compatible.</p>
<a id="more"></a>
<h2 id="Hard-coding-links"><a href="#Hard-coding-links" class="headerlink" title="Hard coding links"></a>Hard coding links</h2><p>Everyones done it. You’re writing a client to an API, and you know that the URL to get a users profile is “/api/user/:id”. So why would you jump through the hoops of requesting the base page to be told this URL every time? It’s just extra overhead for no benefit, so you decide to cut out the middle man and just hard-code the URL. And, of course, it works. Why wouldn’t it? It works great up until the server decides that user profiles should now live on “/api/user/:id/profile”, because they want to put some other user specific details as sub-resources of “/api/user/:id”.</p>
<p>In the HATEOAS world, this is trivial. You change the URLs, update the base page, and everything just works. But this client isn’t in the HATEOAS world, and so doesn’t know that things have changed. So the servers trivial change has just unwittingly broken this client. And because in the HATEOAS world this isn’t a big deal, there was no big announcement about URL patterns changing, so the client is left floundering until someone realises what’s happened.</p>
<h2 id="Unsupported-requirements"><a href="#Unsupported-requirements" class="headerlink" title="Unsupported requirements"></a>Unsupported requirements</h2><p>The HATEOAS world depends on the server providing URLs for everything that the client could ever want to do. But what if there are things that the client wants to do that the server didn’t think of?</p>
<p>Surely though, if the server didn’t think of it then it’s not possible to do it. Except that in some cases that’s not quite true. The main example of this I’m thinking is pagination. It’s quite common for servers to provide URLs for the first, last, next and previous pages. It’s less common for servers to provide URLs for every possible page, and it’s (almost) unheard of for servers to provide URLs for every combination of page offset, page size, sort field, and so on. What I have seen done is the use of <a href="https://tools.ietf.org/html/rfc6570" target="_blank" rel="external">URL Templates</a> to provide a mechanism by which the client can generate the URLs it wants from the parts that the server provides. But this then gets away from what I consider to be pure HATEOAS and is back to the client generating URLs instead of using the ones provided.</p>
<h2 id="Breaking-changes"><a href="#Breaking-changes" class="headerlink" title="Breaking changes"></a>Breaking changes</h2><p>It happens. Hopefully it’s rare, but it happens. Something changes in the server that isn’t a trivial change of URL, but is a more fundamental change in how things work. The only solution to this is that the client gets updated to cope with it. And this is then back to the world where client and server are coupled together.</p>
<h2 id="Understanding-the-data"><a href="#Understanding-the-data" class="headerlink" title="Understanding the data"></a>Understanding the data</h2><p>This is the real killer. This is the bit that really makes a mockery of HATEOAS. No matter how well structured the server is, or how well behaved the client is. If you do everything perfectly and follow all of the rules exactly, you still end up in the situation where the client needs to understand what the data means to do something with it. Retrieving a users profile as a JSON blob is all well and good, but if you don’t understand the distinction between the fields then it’s worthless. This means that at the end of the day, the client is going to be hand crafted to understand the data that the server provides, so why not hand craft it to understand the API by which the data is being provided? It just makes life easier for both server and client in the long run.</p>
<h2 id="So-what’s-the-alternative"><a href="#So-what’s-the-alternative" class="headerlink" title="So what’s the alternative?"></a>So what’s the alternative?</h2><p>The alternative is very simple, but a lot of people shy away from it. Documentation. Write good documentation for your API, for how to use it, for what it does. Provide this documentation in both human and machine readable format - using something like <a href="http://swagger.io/specification/" target="_blank" rel="external">Swagger</a> or <a href="http://raml.org/" target="_blank" rel="external">RAML</a>, for example. Provide a mechanism by which developers can easily trial the API - using something like <a href="http://swagger.io/swagger-ui/" target="_blank" rel="external">Swagger UI</a> for example. Provide early feedback if you are going to be making large changes.</p>
<p>In short, talk to the developers who are writing clients to your server. You’d be amazed at what you can achieve by doing that.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HATEOAS, for anyone who’s not come across it yet, is an additional constraint added on top of normal REST services whereby the only way that clients ever interact with the server is through Hypermedia that the server provides. The server will provide a single starting point, and every single interaction with the server after that is made only through URLs that the server has previously provided to the client. The advantage here is that the server can change independently of the client, and as long as the client only ever follows the URLs provided to it then everything will continue to work correctly.&lt;/p&gt;
&lt;p&gt;That all sounds fantastic on the face of it. Completely decoupled evolution of server and client sounds almost idyllic.&lt;/p&gt;
&lt;p&gt;Of course, in reality this doesn’t actually work. There are a number of problems with the idea, ranging from the very simple - clients cut corners and just hard-code URLs to specific resources instead of following links - to the very complicated, where changes to the server need to be made that just aren’t backwards compatible.&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="api" scheme="https://sazzer.github.io/blog/tags/api/"/>
    
      <category term="REST" scheme="https://sazzer.github.io/blog/tags/REST/"/>
    
      <category term="HATEOAS" scheme="https://sazzer.github.io/blog/tags/HATEOAS/"/>
    
  </entry>
  
  <entry>
    <title>Designing a GraphQL API</title>
    <link href="https://sazzer.github.io/blog/2016/05/07/Designing-a-GraphQL-API/"/>
    <id>https://sazzer.github.io/blog/2016/05/07/Designing-a-GraphQL-API/</id>
    <published>2016-05-07T09:09:59.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>I’ve just gone through some of the reasons that I don’t like the Relay API requirements for GraphQL, so lets have a go at designing an API that I do like.</p>
<a id="more"></a>
<h2 id="Result-Sets"><a href="#Result-Sets" class="headerlink" title="Result Sets"></a>Result Sets</h2><p>Let’s start with the bits of Relay that I do like, and use those. This means most of the Connection parts of the spec. Specifically the separation of Edges and Connections, and the idea of the Page Info object. I’m in two minds if I want to go for Cursor based pagination, or a simpler form using page numbers or offsets, so I’m going to fall back on <a href="https://en.wikipedia.org/wiki/Robustness_principle" target="_blank" rel="external">Postel’s Law</a> for that. Yes, it adds a little bit of complexity to the server layer, but really not that much and the benefits to the client layer are worth it.</p>
<p>Firstly then, the PageInfo object. The PageInfo object is based on the one from Relay, but with some extra fields included.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">type PageInfo &#123;</div><div class="line">  hasNextPage: Boolean! # False if this is the last page</div><div class="line">  hasPreviousPage: Boolean! # False if this is the first page</div><div class="line">  pageOffset: Int! # The 0-based offset of the first record returned in the entire resultset</div><div class="line">  count: Int! # The total number of records in the entire resultset</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>These extra fields let you power a paginator on your results, because between these and the actual results array you know how many results total, and which results you have.</p>
<p>Next comes an Edge. This is not a simple object, because it’s generic over the actual result type, but the definition will look something like:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">type Edge&lt;T&gt; &#123;</div><div class="line">  resource: T!</div><div class="line">  cursor: ID!</div><div class="line">  offset: Int!</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Note that we have the offset of each individual edge included if the client wishes to make use of it, and we also have a Cursor defined if the client wishes to use Cursor based pagination. This comes back to Postel’s Law, being liberal in which clients we support with our API. You’ll note that there is an Offset here as well as in the PageInfo object. The Offset of the first edge returned should be the same value as the offset in the PageInfo object. This means that you can choose to request every single offset individually if you desire, or you can request one single offset for the entire page and calculate the others yourself. It’s trivial for the server to do this, and means the client can be developed either way as desired.</p>
<p>This then leads on to a Connection. Again, this is not a simple object because it’s generic over the type of Edge to support, but it will look something like:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">type Connection&lt;T&gt; &#123;</div><div class="line">  edges: [Edge&lt;T&gt;!]!</div><div class="line">  pageInfo: PageInfo!</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>All of this gives us an example resultset of:<br><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"posts"</span>: &#123;</div><div class="line">    <span class="attr">"edges"</span>: [</div><div class="line">      &#123;</div><div class="line">        <span class="attr">"resource"</span>: &#123;</div><div class="line">          <span class="attr">"title"</span>: <span class="string">"First Post"</span></div><div class="line">        &#125;,</div><div class="line">        <span class="attr">"cursor"</span>: <span class="string">"b2Zmc2V0OjAK"</span>,</div><div class="line">        <span class="attr">"offset"</span>: <span class="number">0</span></div><div class="line">      &#125;, &#123;</div><div class="line">        <span class="attr">"resource"</span>: &#123;</div><div class="line">          <span class="attr">"title"</span>: <span class="string">"Second Post"</span></div><div class="line">        &#125;,</div><div class="line">        <span class="attr">"cursor"</span>: <span class="string">"b2Zmc2V0OjEK"</span>,</div><div class="line">        <span class="attr">"offset"</span>: <span class="number">1</span></div><div class="line">      &#125;</div><div class="line">    ],</div><div class="line">    <span class="attr">"pageInfo"</span>: &#123;</div><div class="line">      <span class="attr">"hasNextPage"</span>: <span class="literal">true</span>,</div><div class="line">      <span class="attr">"hasPreviousPage"</span>: <span class="literal">false</span>,</div><div class="line">      <span class="attr">"pageOffset"</span>: <span class="number">0</span>,</div><div class="line">      <span class="attr">"count"</span>: <span class="number">17</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="Pagination"><a href="#Pagination" class="headerlink" title="Pagination"></a>Pagination</h2><p>Ok then. We can get paginated results out of the API now, but how do we ask for them? We need to have support for asking for a particular page of results. Generally speaking, there are three ways that pagination can work:</p>
<ul>
<li>Cursor + Count + Direction</li>
<li>Page Number + Count</li>
<li>Initial Record Offset + Count</li>
</ul>
<p>If we consider that Count + Direction can be replaced simply by a Count that can be negative, and if we consider that there <em>might</em> - there probably isn’t, but let’s be generic enough to support it anyway - be a reason to support the number of records before a given page or record offset, then we end up in the situation where we need to specify some representation of the start of the page, and the number of records to retrieve.</p>
<p>Unfortunately, GraphQL doesn’t currently allow you to use Unions or Interfaces on Input Types, which means that if we want to support a situation where we support one of a number of different ways of specifying the start of the page we need to do it with custom server-side validation. As such, the schema could look like:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">type Page &#123;</div><div class="line">  cursor: ID</div><div class="line">  page: Int</div><div class="line">  offset: Int</div><div class="line">  count: Int</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>The other way of doing it would be to provide a value and an enum that tells the server what the value means, but then the value needs to be provided as a string even if it’s got to be numeric, and it means that you’ve got to pass an extra value across every time regardless. I also made the schema above allow the count to be optional as well - if not provided it would just use a sensible default instead. Another sensible behaviour would be that if none of cursor, page or offset were provided then the default is the first page of results, and if multiple are provided but happen to refer to the same start point then this is accepted as well. It is only an error when it is ambiguous what is meant.</p>
<h2 id="Mutations"><a href="#Mutations" class="headerlink" title="Mutations"></a>Mutations</h2><p>Next, lets think about Mutations. Mutations in GraphQL are much closer to an RPC concept than in REST. REST pretty much mandates that the only mutations you can do are Create, Update and Delete of resources. GraphQL lets you specify any mutation that you want. However, odds are you want to support the standard Create, Update and Delete operations on most of your resources anyway. For top level resources, this is easy. For nested resources this is a bit more of a challenge, because you can’t nest mutations. As such, you need some way of specifying the nested resource that you want to work with. The proposal here is to define a type to identify the resource in question, which can include a number of different resource identifiers to work down to the nested resource. I would also recommend including inside of this type an optimistic lock value for the resource being edited. You probably don’t need this for all of the resources in the graph, but that depends on your data model.</p>
<p>As an example of this, consider a blog system. You have posts, and you have comments. Comments are not a top-level concept - they make no sense to exist on their own. They are only ever a child of a post. As such, in order to edit a post itself you might use:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">mutation editPost &#123;</div><div class="line">  editPost(id: &#123;post: 1, version: 7&#125;, patch: &#123;title: &apos;The new Title&apos;) &#123;</div><div class="line">    id,</div><div class="line">    version,</div><div class="line">    title,</div><div class="line">    body</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>And to edit a comment you might use:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">mutation editComment &#123;</div><div class="line">  editCommentOnPost(id: &#123;post: 1, comment: 5, version: 2&#125;, patch: &#123;text: &apos;This is the updated comment&apos;&#125;) &#123;</div><div class="line">    id,</div><div class="line">    version,</div><div class="line">    text</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>The first of these updates the post with an ID of 1, providing the version of the post as being 7. The second of these updates the comment with the ID of 5, that is itself a child of the post with an ID of 1, providing the version of the comment as being 2.</p>
<p>You might have noticed that I’ve included the changes as a patch, rather than providing the entire resource. I think this makes a lot of sense in a system that is deliberately designed to minimise the amount of traffic between client and server. If I need to change only one field, I shouldn’t be required to provide every field to the server. I did consider using the JSON Patch standard for this, but you lose type safety and the schema guarantees that make GraphQL so powerful if you do that. You would also need to provide every field as a string and do appropriate type conversions on the server depending on the field you are writing to, which is just a bit ugly. By defining a custom patch format for each resource the schema will do a lot of work for you, and the server updates become really simple.</p>
<h2 id="Error-Handling"><a href="#Error-Handling" class="headerlink" title="Error Handling"></a>Error Handling</h2><p>Eventually when working with an API, something is going to go wrong. It’s generally more likely that mutations will fail than queries, but technically anything could. As such, the concepts here are best suited to mutations but can be made to work with queries if needed.</p>
<p>GraphQL already has support for a top-level errors object returned as part of the response payload. This is wonderful for errors with the incoming request itself - e.g. if it is malformed or invalid in some way, or if the authorization details are invalid. It doesn’t work well when the errors are with a specific part of the request though - e.g. an optimistic lock failure on a mutation. <a href="https://medium.com/@tarkus/validation-and-user-errors-in-graphql-mutations-39ca79cd00bf#.fljypia9y" target="_blank" rel="external">Konstantin Tarkus has a proposal</a> on a way to improve this, but I personally don’t think it goes quite far enough. The basic idea is that the field would have a response that includes the actual response data and a list of errors.</p>
<p>I’m going to propose something very similar, but leveraging the GraphQL type system some more to make it a bit cleaner to work with. My idea here is that the field will have a response that is a union type, consisting of either the actual success response or an error response. The error response would then be a list of error objects, which are richer than just a string error message - because there are extra details that are important as well. This would look something like:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">type Errors &#123;</div><div class="line">  globalErrors: [GlobalError!]!</div><div class="line">  fieldErrors: [FieldErrors!]!</div><div class="line">&#125;</div><div class="line"></div><div class="line">type GlobalError &#123;</div><div class="line">  code: ID!</div><div class="line">  message: String!</div><div class="line">&#125;</div><div class="line"></div><div class="line">type FieldError &#123;</div><div class="line">  code: ID!</div><div class="line">  field: ID!</div><div class="line">  message: String!</div><div class="line">&#125;</div><div class="line"></div><div class="line">union EditCommentResponse = Comment | Errors</div></pre></td></tr></table></figure></p>
<p>This does start to mean that we need to use fragment operators to actually call the mutation, but that’s fine. We need to do something to know if we got a success or a failure anyway, and doing it in the GraphQL call is really no different to doing it in the actual client code. Our edit comment mutation from above now becomes:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">mutation editComment &#123;</div><div class="line">  editCommentOnPost(id: &#123;post: 1, comment: 5, version: 2&#125;, patch: &#123;text: &apos;This is the updated comment&apos;&#125;) &#123;</div><div class="line">    ... on Comment &#123;</div><div class="line">      id,</div><div class="line">      version,</div><div class="line">      text</div><div class="line">    &#125;</div><div class="line">    ... on Errors &#123;</div><div class="line">      globalErrors &#123;</div><div class="line">        code</div><div class="line">      &#125;,</div><div class="line">      fieldErrors &#123;</div><div class="line">        code,</div><div class="line">        field</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>I specified the error codes as an ID, but they could equally well be an enumeration if you want to define in the schema what the possible errors are. That makes it easier to work out whats going on, but at the cost of inflating the schema definition with all of these error enumerations. I also specified the Field Errors as a list of objects that contain the field name. I could equally make it an object with a list of errors, and be more typesafe about the error codes that are supported - it makes no sense to state that a boolean field was too long, for example - but again at the cost of the schema size. This is all a balancing act at the end of the day.</p>
<p>I also did not request the error message in the response. This is because the error code should be enough for the client to understand, and the error message is more aimed at developers who are still getting to grips with the system. The reasoning behind this is i18n - error codes can very easily be converted into localised error messages for the user, but if the client depends on the error message coming from the server then the server needs to have support for every single language that every single possible client could support, which is difficult if you are aiming to support third party clients.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>That’s plenty to be getting on with. It gives us a good basis for an API, but without being too restrictive in what you must and mustn’t do.</p>
<p>One of the key parts of all of this is the fact that any API definition is specific to the application in question. There has been a lot of work recently in defining API standards that are generic enough to support any API - using things like HATEOAS, for example - and I just feel that this is doomed from the offset. There is far too much API specific knowledge that needs to be baked in to the client for a general purpose API to actually be worth the costs involved in using it. You’re better off just developing a more specific API that fits your exact purposes, documenting it well, and being done with it. Clients are going to be custom anyway, so the use of a general purpose API is lost.</p>
<p>As such, if you’ve read through all of this and decide that you like some bits and not others, just use those bits. Do whatever makes the most sense at the time and actually produce something that works. Don’t fall into my usual trap of trying to engineer the perfect API and ending up without actually producing anything.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I’ve just gone through some of the reasons that I don’t like the Relay API requirements for GraphQL, so lets have a go at designing an API that I do like.&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="graphql" scheme="https://sazzer.github.io/blog/tags/graphql/"/>
    
      <category term="relay" scheme="https://sazzer.github.io/blog/tags/relay/"/>
    
      <category term="api" scheme="https://sazzer.github.io/blog/tags/api/"/>
    
  </entry>
  
  <entry>
    <title>I don&#39;t like Relay</title>
    <link href="https://sazzer.github.io/blog/2016/05/06/I-don-t-like-Relay/"/>
    <id>https://sazzer.github.io/blog/2016/05/06/I-don-t-like-Relay/</id>
    <published>2016-05-06T17:48:49.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>I’ve started playing with GraphQL again recently, having been looking into various other specifications for HTTP based APIs, and I’ve come to a conclusion. I really like GraphQL, but I don’t like Relay.</p>
<p>Now, don’t get me wrong, there are bits of Relay that I think are really good ideas. But there are also bits that I just think are either a waste of time, or worse actually make the API harder to use.</p>
<a id="more"></a>
<p>I understand that a lot of the ideas behind Relay are well founded, and they are used to make it so that a single Client side library can be used with any form of Server side API that plays by the rules. And that’s fantastic. Except that it’s incredibly unlikely that you’re ever going to be writing any app that’s got any level of complexity and <em>not</em> need some level of API understanding built into your client.</p>
<p>Let’s start with the bits of Relay that I’m not keen on. It’s not all of it by any means but there are certainly enough to ensure that any GraphQL APIs that I implement will not be properly Relay compliant - Sorry!</p>
<p>The simplest is the concept of “clientMutationId”. This literally has no point. GraphQL lets you, on the client side, provide an alias for any query or mutation field that you are executing. You can therefore use this functionality to provide a client-side ID for any of your mutations and it will automatically work. There is no need to burden the API itself with knowledge of this, which actually is extended to the fact that any mutations need to copy the incoming value to the return value in order for the client to work - something that is automatically handled by all GraphQL libraries when you use field aliasing.</p>
<p>Next is forcing the mutations to only ever take a single argument that is an InputObject, inside of which are all of the actual parameters you want for your mutation. I get that this is here to make writing the client easier, because all of a sudden your client layer is only ever passing across one value, but it’s also relatively pointless and only aims to make understanding the API slightly harder. Admittedly, most of the time I’d probably actually follow this pattern, but there are going to be times when I want multiple parameters and I don’t want to be forced to wrap them in another object just to make the clients happy. (One example of this might be a single upsert mutation. I can have an argument representing the data to create or update, and an optional second argument representing the ID of the record to update - left out if I’m doing a create). There is also the fact here that, unless I’m mistaken, arguments can have default values whereas fields can’t. I can imagine that the requirement to only have one argument means that you need to work out the default values manually in your handler code, instead of having the GraphQL schema do it for you.</p>
<p>Finally, the thing that I like the least about Relay. The “node” query field. This really does nothing more than add complication to the API. And quite a lot of it at that. The idea behind this query field is that you can resolve any arbitrary resource across the entire API by a single call to this. What you need to implement to support this is:</p>
<ul>
<li>An ID generation mechanism that encapsulates the ID and Type of a resource</li>
<li>An Interface that every single resource needs to implement</li>
<li>Some mechanism of actually resolving any arbitrary resource at the top level - which goes against the “Graph” part of “GraphQL”</li>
</ul>
<p>And the benefits of this are that you can load the resource knowing only the ID. Except that you also need to know what type of resource you are loading, because you need to specify a fragment specifier for the fields that you want back. So actually, the only thing we gain from this complexity on the server is additional complexity on the client. Essentially it means that a query looks like:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  findUser: node(id: 'dXNlcjo0Cg==') &#123;</div><div class="line">    ... on User &#123;</div><div class="line">      name</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>instead of looking like:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  findUserById(id: 4) &#123;</div><div class="line">    name</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>So, what <em>do</em> I like from the Relay spec? Pretty much the only things that are left - namely the Cursor Connections topic. The idea behind the Edges and Connections is quite nice I feel, separating out the resultset from the actual results in it. The way that Cursors work, and are used for pagination is also quite nice if you plan on doing Cursor based pagination - but the fact that Relay pretty much enforces Cursor based pagination is less nice. (Cursor based pagination is great if you are only ever going to be scrolling through. It’s useless if you want to be able to jump to arbitrary records/pages since it gives no easy facility to do this). The standardised PageInfo object works well as well, meaning that there is a common knowledge about how you can manage expectations here.</p>
<p>And so, based on all of that, next time I’m going to be putting forward a proposal for a GraphQL based API that I feel works much better from both client and server points of view.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I’ve started playing with GraphQL again recently, having been looking into various other specifications for HTTP based APIs, and I’ve come to a conclusion. I really like GraphQL, but I don’t like Relay.&lt;/p&gt;
&lt;p&gt;Now, don’t get me wrong, there are bits of Relay that I think are really good ideas. But there are also bits that I just think are either a waste of time, or worse actually make the API harder to use.&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="graphql" scheme="https://sazzer.github.io/blog/tags/graphql/"/>
    
      <category term="relay" scheme="https://sazzer.github.io/blog/tags/relay/"/>
    
      <category term="api" scheme="https://sazzer.github.io/blog/tags/api/"/>
    
  </entry>
  
  <entry>
    <title>What is Relay?</title>
    <link href="https://sazzer.github.io/blog/2016/01/24/What-is-Relay/"/>
    <id>https://sazzer.github.io/blog/2016/01/24/What-is-Relay/</id>
    <published>2016-01-24T22:22:28.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>Having just had a very brief overview of GraphQL, the next thing that might be of interest is Relay. Relay in this sense means one of two things. Either it means the <a href="https://facebook.github.io/relay/" target="_blank" rel="external">Relay Javascript Framework</a> that is used in your frontend layer to communicate with a GraphQL Server, or - and this is what’s more interesting to myself right now - it means the set of concepts that your GraphQL Server needs to adhere to in order for it to be Relay Compatible.</p>
<a id="more"></a>
<p>Ultimately, Relay defines three specific concepts that you need to follow in your schema definition. However, one of these concepts is quite sizable and I tend to think of it more as four concepts that have some overlap. These four concepts are:</p>
<ul>
<li>Entity Loading</li>
<li>Connections between Entities</li>
<li>Pagination</li>
<li>Mutations</li>
</ul>
<h3 id="Entity-Loading"><a href="#Entity-Loading" class="headerlink" title="Entity Loading"></a>Entity Loading</h3><p>This is covered in the Relay Specification under <a href="https://facebook.github.io/relay/graphql/objectidentification.htm" target="_blank" rel="external">Object Identification</a>. </p>
<p>In brief, Relay requires that all of your identifiers for all of your resources are globally unique, and a single query field that can be used to load any resource of any time from this globally unique identifier. </p>
<p>When we talk about a Globally Unique Identifier, we do really mean it. It must not be possible for the server to be ambiguous between the kind of resource to load when it is given just the ID. How you achieve this is entirely up to you, but my preference here is to have the ID be a Base64 encoded representation of the Resource Name and Database ID. (The idea of Base64 encoding the ID is so that it’s obvious to the client that it’s just an Opaque ID). So, for example:</p>
<ul>
<li>The first User might have an ID of “dXNlcjoxCg==” (user:1)</li>
<li>The first Film might have an ID of “ZmlsbToxCg==” (film:1)</li>
</ul>
<p>Notice here that even though the database IDs are both the same, the ID that is used over the API is totally different, and when the server receives this ID it has enough information to know whether it represents a User or a Film.</p>
<p>The other part of this concept is that there is a single uniform way of loading any resource knowing only it’s ID. This is the Query field <em>node</em>. This works by returning an instance of the Node interface - all resources in Relay must implement the Node interface, though all that means is that they have a unique ID - and uses some more complicated parts of the GraphQL specification to be able to extract the desired fields from it - namely the use of Inline Fragments to extract fields from a specific subtype of Node. This means that we can write a query as follow:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">query GetFilm &#123;</div><div class="line">    node(id: 'ZmlsbToxCg==') &#123;</div><div class="line">        id,</div><div class="line">        ... on Film &#123;</div><div class="line">            name</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>What this query says is “Get the resource with an ID of ‘ZmlsbToxCg==’, returning the ID and - iff the resource is a Film - the Name of the film”. We know that it’s a Film, because somehow we got hold of the ID and when we got hold if it we will have known it’s a Film from the context. This does mean that you need to keep track of what each ID means on the client side or else you’ll get very confused, but that shouldn’t be too difficult.</p>
<h3 id="Connections-between-Entities"><a href="#Connections-between-Entities" class="headerlink" title="Connections between Entities"></a>Connections between Entities</h3><p>This is covered in the Relay Specification under <a href="https://facebook.github.io/relay/graphql/connections.htm" target="_blank" rel="external">Cursor Connections</a>.</p>
<p>Relay requires a particular pattern when one resource links to a collection of other resources. This is represented by the concept of a Connection and Edges, where the outermost resource contains a Connection, the Connection contains a list of Edges, and each Edge contains a Node. For example, the User resource may look like:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    id: 'dXNlcjoxCg==',</div><div class="line">    name: 'Graham',</div><div class="line">    filmConnection: &#123;</div><div class="line">        edges: [</div><div class="line">            &#123;</div><div class="line">                node: &#123;</div><div class="line">                    id: 'ZmlsbToxCg==',</div><div class="line">                    name: 'The Shawshank Redemption'</div><div class="line">                &#125;</div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                node: &#123;</div><div class="line">                    id: 'ZmlsbToyTCg==',</div><div class="line">                    name: 'The Godfather'</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>The benefit out of this is that the Connection and Edge entries can contain other data that relates to the related edges, and then the Node is the actual resource that was wanted. Specifically, the mandated extra information that can be included on Connection and Edge are related to Pagination, but you can put anything you like there. Specifically it can make sense to put the total number of records on the Connection.</p>
<p>Note that each Node here has an ID on it. This must be usable in a call to the <em>node()</em> query - described above - to re-request the same resource again, possibly with different fields this time.</p>
<h3 id="Pagination"><a href="#Pagination" class="headerlink" title="Pagination"></a>Pagination</h3><p>This is covered in the Relay Specification under <a href="https://facebook.github.io/relay/graphql/connections.htm" target="_blank" rel="external">Cursor Connections</a>.</p>
<p>Pagination builds on top of the Connections and Edges concepts by adding some mandatory fields to the response, and some mandatory attributes to the queries that allow you to make use of them.</p>
<p>On the Connection, we add a field called <em>pageInfo</em> which contains some details about the current page of records that was returned. This must contain fields <em>hasNextPage</em> and <em>hasPreviousPage</em>, but can contain other fields as well if that makes sense to your pagination model. On the Edge, we add a field called <em>cursor</em> which allows us to refer to this exact point in the Connection. This means that the above can then become:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    id: 'dXNlcjoxCg==',</div><div class="line">    name: 'Graham',</div><div class="line">    filmConnection: &#123;</div><div class="line">        pageInfo: &#123;</div><div class="line">            hasNextPage: true,</div><div class="line">            hasPreviousPage: false</div><div class="line">        &#125;,</div><div class="line">        edges: [</div><div class="line">            &#123;</div><div class="line">                cursor: 'MQo=',</div><div class="line">                node: &#123;</div><div class="line">                    id: 'ZmlsbToxCg==',</div><div class="line">                    name: 'The Shawshank Redemption'</div><div class="line">                &#125;</div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">                cursor: 'Mgo=',</div><div class="line">                node: &#123;</div><div class="line">                    id: 'ZmlsbToyTCg==',</div><div class="line">                    name: 'The Godfather'</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>The values returned in the <em>cursor</em> field are used for requesting more results. The second part of the Pagination concept is that there are certain required query attributes that you need to include to request a page of results. There are four additional attributes that are required to be supported, though only in certain combinations:</p>
<ul>
<li>first - The number of records to return reading <strong>forward</strong> from the <em>after</em> cursor, or the start of the resultset if not specified.</li>
<li>after - The cursor to start reading forward from, if <em>first</em> was also specified.</li>
<li>last - The number of records to return reading <strong>backwards</strong> from the <em>before</em> cursor, or the end of the resultset if not specified.</li>
<li>before - The cursor to start rreading backwards from, if <em>last</em> was also specified.</li>
</ul>
<p>It is technically possible to specify both <em>first</em> and <em>last</em>, but it’s not obvious what should happen if you do. Note that none of these attributes are required, and if none of them are set then the specification states that the entire resultset should be returned. It may make more sense to instead return a default subset - e..g the first 10 records - so that the payload and server processing is not overwhelming.</p>
<p>For example, requesting the above user with the next 2 films would look like:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">query UserFilms &#123;</div><div class="line">    node(id: "dXNlcjoxCg==") &#123;</div><div class="line">        ... on User &#123;</div><div class="line">            id,</div><div class="line">            name,</div><div class="line">            filmConnection(first: 2, after: "ZmlsbToyTCg==") &#123;</div><div class="line">                ........</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="Mutations"><a href="#Mutations" class="headerlink" title="Mutations"></a>Mutations</h3><p>This is covered in the Relay Specification under <a href="https://facebook.github.io/relay/graphql/mutations.htm" target="_blank" rel="external">Input Object Mutations</a>.</p>
<p>The only thing that the Relay Specification mandates is that each mutation must accept only a single argument, named <em>input</em>, and that this input must support a field called <em>clientMutationId</em>. The response from the mutation must return the exact same value in an output field also called <em>clientMutationId</em>. All of the rest of the inputs to the mutation are just included as fields on the <em>input</em> argument.</p>
<h3 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together"></a>Putting it all together</h3><p>That seems like a lot that needs to be done. It’s not quite as bad as it seems though, since you only <em>need</em> to do these if you are wanting to support Relay as a client layer, and you may decide that you want to implement them anyway. In particular, the Connections and Edges are a useful way of grouping together resources in a graph without introducing differences in the resources when they are present at different parts of the graph.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Having just had a very brief overview of GraphQL, the next thing that might be of interest is Relay. Relay in this sense means one of two things. Either it means the &lt;a href=&quot;https://facebook.github.io/relay/&quot;&gt;Relay Javascript Framework&lt;/a&gt; that is used in your frontend layer to communicate with a GraphQL Server, or - and this is what’s more interesting to myself right now - it means the set of concepts that your GraphQL Server needs to adhere to in order for it to be Relay Compatible.&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="graphql" scheme="https://sazzer.github.io/blog/tags/graphql/"/>
    
      <category term="relay" scheme="https://sazzer.github.io/blog/tags/relay/"/>
    
      <category term="api" scheme="https://sazzer.github.io/blog/tags/api/"/>
    
  </entry>
  
  <entry>
    <title>What is GraphQL?</title>
    <link href="https://sazzer.github.io/blog/2016/01/24/What-is-GraphQL/"/>
    <id>https://sazzer.github.io/blog/2016/01/24/What-is-GraphQL/</id>
    <published>2016-01-24T18:24:12.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>You may or may not have already come across GraphQL by now. GraphQL is a new concept coming out of the software people at Facebook, and like their other ideas recently - React and Flux mainly - it looks like it could really change the way we think about our HTTP based API design.</p>
<p>So, what is it exactly? And why is it a big deal?</p>
<a id="more"></a>
<p>GraphQL is a way of structuring your API such that the API Client is more in control of exactly what information is requested and how the information is returned. It also allows much more structured data to be returned in far fewer requests, because of how it represents the data that is being retrieved.</p>
<p>That all sounds very good, but what does it mean? And how does it compare to what we’re already used to?</p>
<p>Let’s start with the second question and go from there. It seems that the go-to mechanism for writing HTTP based APIs will be something resembling REST, often with all kinds of extra bits added on as well (HAL, HATEOAS, etc). REST gives you a hierarchical API pattern, in which you specify Resources and allow the client to access these resources in various different ways. Generally speaking, this breaks down to:</p>
<ul>
<li>List/Search - GET /resource</li>
<li>Retrieve - GET /resource/1</li>
<li>Create - POST /resource</li>
<li>Update - PUT /resource/1</li>
<li>Delete - DELETE /resource/1</li>
</ul>
<p>The hierarchical nature of this allows for some flexibility in the URL patterns to reflect how the data is structured. Let’s be a bit more realistic now and use a more real-world example. I’m going for a DVD Catalogue, since it’s a bit more unusual than what you normally see. In this, we have the following resources available:</p>
<ul>
<li>/users - Access to the user database</li>
<li>/films - Access to the film database</li>
<li>/users/&lt;id&gt;/films - Access to the film database for a particular user</li>
</ul>
<p>So far, so good. Although already we can see that we need to do extra work if we want to return a list of users and the films that each user has. And what happens if we want to see a list of other users that also have a particular film? The obvious way of doing that is:</p>
<ul>
<li>/films/&lt;id&gt;/users - Access to the users that all own a particular film</li>
</ul>
<p>This works, but again it makes it a bit clunky to see details across a number of films in one go. And what about when you want to find users that own films that are owned by a particular user? That’s quite a nasty case, but it’s perfectly feasible. In order to do that, we would need to:</p>
<ol>
<li>GET /users - find the user in question.</li>
<li>GET /users/&lt;id&gt;/films - Find the films that the user has.</li>
<li>GET /films/&lt;id&gt;/users - Find the users for each film returned. </li>
</ol>
<p>This means that if we do a search in #1 that returns 15 users, and each user has an average of 10 films, in order to find all of the users with shared films we have to:</p>
<ul>
<li>Make request #1 once</li>
<li>Make request #2 15 times - once per user</li>
<li>Make request #3 150 times - once for each film for each user</li>
</ul>
<p>That’s a total of 166 requests to get this data. Of course, if it’s a common use case you could write a custom endpoint to do it all in a single request, but then you are writing server-side code to handle client-side concerns.</p>
<p>Enter GraphQL. When you write a GraphQL API, you don’t think of each resource as being independent any more. You need to instead think of all the resources as being an inter-connected graph. You define some entry points into the graph, and then the client can navigate as they wish. This all sounds very good, but what does it look like? The above query - to find all of the users that share films with another set of users - could look like:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">query UsersSharingFilms &#123;</div><div class="line">  users() &#123;</div><div class="line">    id,</div><div class="line">    name,</div><div class="line">    films() &#123;</div><div class="line">      id,</div><div class="line">      name,</div><div class="line">      ownerBy() &#123;</div><div class="line">        id,</div><div class="line">        name</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>So, that’s one query. That means one single request to the server, and it will return:</p>
<ul>
<li>All of the matching users, including the ID, Name and Films they own</li>
<li>For each Film, the ID, Name and the Users that own the film</li>
<li>For each owner, the ID and Name.</li>
</ul>
<p>Notice as well how there is a nesting from User -&gt; Film -&gt; User, which is where the Graph part comes in. Because the client dictates the query to be executed, and the server just provides the graph schema to query, you can navigate this as deeply or widely as you want.</p>
<p>Also notice how the query is specifying which fields to return. This means that you only ever retrieve the details that you actually care about, and nothing more. In a RESTful API, you may find that the server returns significantly more information than you care about, just in case you do want it. This then takes longer to produce the response, longer to transmit the response and longer to parse the response. (Generally speaking, the time we’re talking about here is negligible, but as the resultset grows in size, this has more impact)</p>
<p>Now, what happens if you want to get only a single user instead? You do this exactly the same:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">query GetUser &#123;</div><div class="line">    user(id: &lt;id&gt;) &#123;</div><div class="line">        id,</div><div class="line">        name</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>In fact, all Query mechanisms work exactly the same. You use a server-defined query that gives you the entry point into the graph, and you go from there. What’s even better, the top level request in the query works exactly the same as all other levels, so you can actually specify multiple queries in a single request:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">query GetUsersAndFilms &#123;</div><div class="line">    users() &#123;</div><div class="line">        id,</div><div class="line">        name</div><div class="line">    &#125;,</div><div class="line">    films() &#123;</div><div class="line">        id,</div><div class="line">        name</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>This query will return a response containing to lists, one of all the users and the other of all the films. And again, this is one request and one response.</p>
<p>This all means that you can reproduce everything that REST does with a GET, but with more flexibility and more control. But what about changing data? How are we meant to do anything that causes the data to change?</p>
<p>That’s simple. GraphQL defines two different top-level options. Above we’ve seen Query, but the other option is Mutation. This works identically to Query, but it has the understanding that the data will be mutated as a side-effect of the query. From the client point of view, the only difference is that you use the “mutation” keyword instead of the “query” keyword, and you need to use a defined Mutation instead of a defined Query. Because of this, we have the added benefits that we can specify multiple mutations in a single request, and we can specify the fields that we want to get back in the response to the mutation. For example:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">mutation AddFilmsToUser &#123;</div><div class="line">    addFilmToUser(user: &lt;userId1&gt;, film: &lt;filmId1&gt;) &#123;</div><div class="line">        name,</div><div class="line">        dateAdded</div><div class="line">    &#125;,</div><div class="line">    addFilmToUser(user: &lt;userId2&gt;, film: &lt;filmId2&gt;) &#123;</div><div class="line">        name,</div><div class="line">        dateAdded</div><div class="line">    &#125;,</div><div class="line">    addFilmToUser(user: &lt;userId3&gt;, film: &lt;filmId3&gt;) &#123;</div><div class="line">        name,</div><div class="line">        dateAdded</div><div class="line">    &#125;,</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Now, all this sounds fantastic, but how do you know what you can do with the API? One of the remaining problems that REST APIs have that GraphQL helps to fix is working out what you can actually do with the API. GraphQL offers a standard set of query fields that you can use to query the actual GraphQL Schema as opposed to the API Itself. This means that you can as a GraphQL Service to describe itself, and you can then have documentation and API Clients automatically generated based on this schema definition. This is actually quite complicated so I’m not going to describe it here, but there’s a fair few details to find already.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;You may or may not have already come across GraphQL by now. GraphQL is a new concept coming out of the software people at Facebook, and like their other ideas recently - React and Flux mainly - it looks like it could really change the way we think about our HTTP based API design.&lt;/p&gt;
&lt;p&gt;So, what is it exactly? And why is it a big deal?&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="graphql" scheme="https://sazzer.github.io/blog/tags/graphql/"/>
    
      <category term="api" scheme="https://sazzer.github.io/blog/tags/api/"/>
    
  </entry>
  
  <entry>
    <title>Some notes on supporting CORS</title>
    <link href="https://sazzer.github.io/blog/2015/10/24/Some-notes-on-supporting-CORS/"/>
    <id>https://sazzer.github.io/blog/2015/10/24/Some-notes-on-supporting-CORS/</id>
    <published>2015-10-24T09:57:17.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>If you didn’t know, <a href="http://www.w3.org/TR/cors/" target="_blank" rel="external">CORS</a> - or Cross-Origin Resource Sharing - is a W3C specification for allowing a page loaded from one domain to access resources on a different domain via XMLHttpRequest calls. This is important for a number of reasons, but the main one for me right now is because it means that you can split your application into multiple smaller services - microservices - that are hosted on different subdomains and still access them all from your Javascript.</p>
<a id="more"></a>
<p>So how does it work? There’s a lot of good details about it already on the web, so I won’t go into too many details. Briefly though, when you make an XMLHttpRequest call, the <em>response</em> contains some special headers that indicate if it is acceptable for the browser to consume the data.</p>
<p>Yes. The Response. What this means is that the request may go through regardless, but the response might not always make it back to the browser. I’ve just tested this and can conform that this is exactly what happens.</p>
<p>Now, of course, there is more to it than that. There are two types of Cross-Origin Requests that can be made - Simple Cross-Origin Requests and Cross-Origin Requests with Preflight. </p>
<p>A Simple request is one that follows a set of rules, which state:</p>
<ul>
<li>The HTTP method is one of GET, HEAD or POST</li>
<li>The custom headers on the request only include Accept, Accept-Language, Content-Language and Content-Type</li>
<li>If a Content-Type header is included, the value is one of application/x-www-form-urlencoded, multipart/form-data, or text/plain</li>
</ul>
<p>If all of these rules are met then the browser will make the request, the server will receive and process the request, and the browser may or may not receive the response depending on the presence of CORS headers in the response.</p>
<p>If any of these rules are not met then the browser will instead make a Pre-Flight request first - which is an OPTIONS request to the same URL, and if the Pre-Flight response contains the appropriate CORS headers then the browser will make the actual request.</p>
<p>All seems well and good. We now have a mechanism by which we can write a web application such that the browser makes requests to a selection of servers and it all works correctly.</p>
<p>For more details on how all of this works, the <a href="http://www.w3.org/TR/cors/" target="_blank" rel="external">HTML5 Rocks page</a> has a lot of good details.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;If you didn’t know, &lt;a href=&quot;http://www.w3.org/TR/cors/&quot;&gt;CORS&lt;/a&gt; - or Cross-Origin Resource Sharing - is a W3C specification for allowing a page loaded from one domain to access resources on a different domain via XMLHttpRequest calls. This is important for a number of reasons, but the main one for me right now is because it means that you can split your application into multiple smaller services - microservices - that are hosted on different subdomains and still access them all from your Javascript.&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="webdev" scheme="https://sazzer.github.io/blog/tags/webdev/"/>
    
      <category term="cors" scheme="https://sazzer.github.io/blog/tags/cors/"/>
    
      <category term="ajax" scheme="https://sazzer.github.io/blog/tags/ajax/"/>
    
  </entry>
  
  <entry>
    <title>Unit Testing ES6 Modules - Mockery</title>
    <link href="https://sazzer.github.io/blog/2015/08/20/Unit-Testing-ES6-Modules-Mockery/"/>
    <id>https://sazzer.github.io/blog/2015/08/20/Unit-Testing-ES6-Modules-Mockery/</id>
    <published>2015-08-20T20:45:10.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>For a long time, one of the big problems I’ve had with NodeJS is that of Unit Testing. Specifically with the fact that, by definition, a Unit Test is testing a single unit in complete isolation of the rest of the world. In Node terms, a Unit would be a single Module. </p>
<a id="more"></a>
<p>In Java, this is solved by the fact that you will often use an <a href="https://en.wikipedia.org/wiki/Inversion_of_control" target="_blank" rel="external">IoC container</a> to wire your application together. That way, every object that you are testing is constructed by passing in the objects that it depends on. That way, in a Unit Test you can simply pass in a <a href="https://en.wikipedia.org/wiki/Mock_object" target="_blank" rel="external">Mock</a> version of the object that is expected, and so you have absolute control of the way the Mock object reacts. This means that you are able to test your object in terms of the API that it’s dependencies expose, instead of needing to worry about implementation details of those other objects. This also means that you can test things that are otherwise hard to test (Ever wanted to test that you’re correctly handling an IOException that realistically will never happen?)</p>
<p>In Node, the presence of the module system means that you write your code the other way around. Instead of wiring up everything from the outside, you tend to write it so that modules pull their dependecies in. There are IoC containers for Node, but they’re often clunky and I’ve yet to find one that works well with ES6 modules. (If anyone knows of one, let me know!). The fact that modules pull in their dependencies means that it’s very difficult to provide a mock version of them instead.</p>
<p>Enter <a href="https://github.com/mfncooper/mockery" target="_blank" rel="external">Mockery</a>. Mockery is a very clever little module that hooks into the Node module system and makes it so that a call to require a module actually does something different. This is evil, but awesome, and instantly means that even though modules pull in their dependencies we can intercept that and give them a version that we control.</p>
<p>Then we come to ES6. If you are using the ES6 module system then you’ll know that the new “import” statements can only go at the top level. This means that your object is imported <em>before</em> you can intercept it with Mockery, and the whole thing falls apart.</p>
<p>So what do we do? Well, as it happens, there’s no reason that we <em>can’t</em> use the old “require” statement in ES6 code, and this statement isn’t restricted to being at the top level like the “import” statement is. This means that we can simply require the module under test after we’ve set Mockery up, and we’re back in business.</p>
<p>This example is written in Mocha. There’s no reason why it can’t apply to anything else though.</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// first.js</span></div><div class="line"><span class="keyword">import</span> &#123;name&#125; <span class="keyword">from</span> <span class="string">"second"</span>;</div><div class="line"></div><div class="line"><span class="keyword">export</span> <span class="function"><span class="keyword">function</span> <span class="title">hello</span>(<span class="params"></span>) </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="string">"Hello, "</span> + name();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// second.js</span></div><div class="line"><span class="keyword">export</span> <span class="function"><span class="keyword">function</span> <span class="title">name</span>(<span class="params"></span>) </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="string">"Graham"</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// first-spec.js</span></div><div class="line"><span class="keyword">import</span> assert <span class="keyword">from</span> <span class="string">"assert"</span>;</div><div class="line"><span class="keyword">import</span> mockery <span class="keyword">from</span> <span class="string">"mockery"</span>;</div><div class="line"></div><div class="line">describe(<span class="string">"first"</span>, () =&gt; &#123;</div><div class="line">    <span class="keyword">let</span> secondMock;</div><div class="line">    <span class="keyword">let</span> first;</div><div class="line"></div><div class="line">    before(() =&gt; &#123;</div><div class="line">        mockery.enable();</div><div class="line">        mockery.registerAllowable(<span class="string">"first"</span>);</div><div class="line"></div><div class="line">        secondMock = &#123;&#125;;</div><div class="line">        mockery.registerMock(<span class="string">"second"</span>, secondMock);</div><div class="line"></div><div class="line">        first = <span class="built_in">require</span>(<span class="string">"first"</span>);</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    after(() =&gt; &#123;</div><div class="line">        mockery.disable();</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    it(<span class="string">"should return the right name"</span>, () =&gt; &#123;</div><div class="line">        secondMock.name = () =&gt; &#123;</div><div class="line">            <span class="keyword">return</span> <span class="string">"Fred"</span>;</div><div class="line">        &#125;;</div><div class="line"></div><div class="line">        assert(<span class="string">"Hello, Fred"</span> === first.hello());</div><div class="line">    &#125;);</div><div class="line">&#125;);</div></pre></td></tr></table></figure>
<p>So, what are we doing here? We have three files, imaginatively named “first”, “second” and “first-spec”. The “first” module explicitly depends on the “second” module, and the “first” module is the one we’re testing. By using Mockery, we override what the “second” module looks like before we load the “first” module. This means that instead of returning a name of “Graham” like it’s meant to, it instead returns a name of “Fred”. </p>
<p>There is a slight oddity in there. Because we’re enabling Mockery and then doing a “require” call for the module “first”, which we haven’t mocked out, we need to tell Mockery that this is allowed otherwise it will log a warning when it runs. It will still work, but it’s a bit noisy, and what’s worse it obscures when we really did miss mocking out a module for real.</p>
<p>In more complicated code you could then use something like <a href="http://sinonjs.org/" target="_blank" rel="external">Sinon</a> to implement your Mocks behaviour, but for the sake of this simple example that was a bit too much.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;For a long time, one of the big problems I’ve had with NodeJS is that of Unit Testing. Specifically with the fact that, by definition, a Unit Test is testing a single unit in complete isolation of the rest of the world. In Node terms, a Unit would be a single Module. &lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="javascript" scheme="https://sazzer.github.io/blog/tags/javascript/"/>
    
      <category term="es6" scheme="https://sazzer.github.io/blog/tags/es6/"/>
    
      <category term="testing" scheme="https://sazzer.github.io/blog/tags/testing/"/>
    
      <category term="mocks" scheme="https://sazzer.github.io/blog/tags/mocks/"/>
    
      <category term="mockery" scheme="https://sazzer.github.io/blog/tags/mockery/"/>
    
  </entry>
  
  <entry>
    <title>Starting a NodeJS Project - Testing</title>
    <link href="https://sazzer.github.io/blog/2015/08/19/Starting-a-NodeJS-Project-Testing/"/>
    <id>https://sazzer.github.io/blog/2015/08/19/Starting-a-NodeJS-Project-Testing/</id>
    <published>2015-08-19T19:32:03.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>I try to be a bit of a stickler when it comes to testing of my code. As such, I always like to make sure that I have support for Unit Testing in my projects from the very start, knowing full well that if not then the test quality just won’t be as good and the project will suffer as a result.</p>
<p>Testing of a project comes in various different flavours, of various different levels of importance. </p>
<ul>
<li>Unit Testing. This is testing every single Unit - Module in NodeJS terms - of a project in complete isolation. Everything that the module depends on should be replaced with Mocks that you have control over, so that you know exactly what is going on.</li>
<li>Integration Testing. This is testing a group of units all together to make sure that the inter-operation between them is correct. The level that you do this at completely depends on the tests that you are doing. It might be simple testing 2 or 3 modules together, or it might be testing a full stack from API all the way down to a (mock) database. The key difference is that it is not testing one Unit in isolation, but testing the integration of several Units together, but isolated from everything outside of that set of Units</li>
<li>Verification Testing. This is a less commonly used term, but it’s one that I use to mean testing of the entire project as a whole. It’s essentially Integration Testing, but where the collection of Units that you are Integration Testing is “all of them”. This can be very hard to get right, because ideally you need to test against a real database engine, and using a real browser. Often it’s just easier to do this level of testing by hand, but if you can automate it then you really should.</li>
</ul>
<a id="more"></a>
<p>For now, I’m going to concentrate only on the first two levels, since they can be covered by the exact same setup and just varies on how you write the tests. If I get around to it I might well write up how I handle the Verification Tests at some point - though that does involve working out a setup that I’m truly happy with first.</p>
<p>I always end up using Mocha for my NodeJS testing. Plugging this in to Grunt is relatively easy, though there are a number of modules that you can use that work better or worse than others. Due to the way that I do <a href="/blog/2015/08/19/Starting-a-NodeJS-Project-ES6/" title="my ES6 setup">my ES6 setup</a>, needing to run the application with the NODE_PATH environment variable already set, this really limits the way that the tests can be run. </p>
<p>The setup that I’ve found works best is <em>grunt-mocha-cli</em>. This is installed by running:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install --save-dev mocha grunt-mocha-cli</div></pre></td></tr></table></figure></p>
<p>The one tricky part with this is that, because of the name of the module, it doesn’t work well with <em>jit_grunt</em>. Thankfully, <em>jit_grunt</em> was written with this fact in mind and gives a way around it. </p>
<p>The Grunt configuration that I use for all of this is as follows:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">require</span>(<span class="string">"jit-grunt"</span>)(grunt, &#123;</div><div class="line">    mochacli: <span class="string">"grunt-mocha-cli"</span></div><div class="line">&#125;);</div><div class="line">................</div><div class="line">mochacli: &#123;</div><div class="line">    node: &#123;</div><div class="line">        options: &#123;</div><div class="line">            reporter: <span class="string">"spec"</span>,</div><div class="line">            growl: <span class="literal">true</span>,</div><div class="line">            env: [</div><div class="line">                targetNodeMainDir, </div><div class="line">                targetNodeTestDir</div><div class="line">            ].join(<span class="string">":"</span>),</div><div class="line">            files: [ </div><div class="line">                path.join(targetNodeTestDir, <span class="string">"**"</span>, <span class="string">"*-spec.js"</span>)</div><div class="line">            ]</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>This setup will treat every file that has been built into the <em>target/node/test</em> directory and has a filename ending with “-spec.js”. This means that, if needed, you can write helper modules that are used by the tests, and as such live under <em>src/node/test</em> instead of <em>src/node/main</em>. All you need to do is ensure that the helper modules don’t have filenames that end with “-spec.js”.</p>
<p>You’ll also notice that we’ve passed in both the Target Node and Target Test directories as the NODE_PATH environment variable. This allows for us to find all of the modules in the main source area, and the modules in the test area as appropriate.</p>
<p>The only thing left to do is to wire it up. This is done simply by changing the task definition to:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">grunt.registerTask(<span class="string">"test"</span>, [<span class="string">"build"</span>, <span class="string">"babel:nodeTest"</span>, <span class="string">"mochacli:node"</span>]);</div></pre></td></tr></table></figure></p>
<p>The end result of this is that whenever you run “grunt test”, and by extension “npm test” we will get all of our tests run. </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I try to be a bit of a stickler when it comes to testing of my code. As such, I always like to make sure that I have support for Unit Testing in my projects from the very start, knowing full well that if not then the test quality just won’t be as good and the project will suffer as a result.&lt;/p&gt;
&lt;p&gt;Testing of a project comes in various different flavours, of various different levels of importance. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unit Testing. This is testing every single Unit - Module in NodeJS terms - of a project in complete isolation. Everything that the module depends on should be replaced with Mocks that you have control over, so that you know exactly what is going on.&lt;/li&gt;
&lt;li&gt;Integration Testing. This is testing a group of units all together to make sure that the inter-operation between them is correct. The level that you do this at completely depends on the tests that you are doing. It might be simple testing 2 or 3 modules together, or it might be testing a full stack from API all the way down to a (mock) database. The key difference is that it is not testing one Unit in isolation, but testing the integration of several Units together, but isolated from everything outside of that set of Units&lt;/li&gt;
&lt;li&gt;Verification Testing. This is a less commonly used term, but it’s one that I use to mean testing of the entire project as a whole. It’s essentially Integration Testing, but where the collection of Units that you are Integration Testing is “all of them”. This can be very hard to get right, because ideally you need to test against a real database engine, and using a real browser. Often it’s just easier to do this level of testing by hand, but if you can automate it then you really should.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="buildsystems" scheme="https://sazzer.github.io/blog/categories/buildsystems/"/>
    
    
      <category term="nodejs" scheme="https://sazzer.github.io/blog/tags/nodejs/"/>
    
      <category term="javascript" scheme="https://sazzer.github.io/blog/tags/javascript/"/>
    
      <category term="grunt" scheme="https://sazzer.github.io/blog/tags/grunt/"/>
    
      <category term="mocha" scheme="https://sazzer.github.io/blog/tags/mocha/"/>
    
      <category term="testing" scheme="https://sazzer.github.io/blog/tags/testing/"/>
    
  </entry>
  
  <entry>
    <title>Starting a NodeJS Project - ES6</title>
    <link href="https://sazzer.github.io/blog/2015/08/19/Starting-a-NodeJS-Project-ES6/"/>
    <id>https://sazzer.github.io/blog/2015/08/19/Starting-a-NodeJS-Project-ES6/</id>
    <published>2015-08-19T05:35:43.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>In the previous post, I set up a new NPM project to work with, and added in Grunt as a task runner so that we can do complex tasks if we want to. Next is setting it up to build ES6 code into ES5 via Babel, so that we get to use the newer features - Classes, Arrow Functions, Destructuring, Let/Const, and so on - whilst running on a runtime that doesn’t yet support them - because no runtime yet supports ES6 fully.</p>
<p><a href="https://babeljs.io/" target="_blank" rel="external">Babel</a> is a Transpiler that we can use to automatically convert ES6 code into ES5 code for exactly this purpose. It literally takes existing ES6 source code as input, and converts it into ES5 source code as output - in the same way that a compiler would covert source code into machine code.</p>
<p>If we so desired, we could use the exact same setup for working with <a href="http://coffeescript.org/" target="_blank" rel="external">CoffeeScript</a>, <a href="https://www.dartlang.org/" target="_blank" rel="external">Dart</a>, <a href="http://www.typescriptlang.org/" target="_blank" rel="external">TypeScript</a>, or any of a <a href="https://github.com/jashkenas/coffeescript/wiki/List-of-languages-that-compile-to-JS" target="_blank" rel="external">growing set of languages</a> that we can convert into Javascript.</p>
<a id="more"></a>
<p>I always set up my projects with a consistent filesystem structure, so that I know what I’m talking about. This is largely stolen from Maven, simply because I know it and it works for me. As such, I work with the following structure:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">| package.json</div><div class="line">| GruntFile.js</div><div class="line">+ src</div><div class="line">  + node // This is where all of the Server-side code goes</div><div class="line">    | main</div><div class="line">    | test</div><div class="line">  + js // This is where all of the Client-side code goes</div><div class="line">    | main</div><div class="line">    | test</div><div class="line">+ target</div><div class="line">  + node // This is where all of the Server-side output goes</div><div class="line">    | main</div><div class="line">    | test</div></pre></td></tr></table></figure></p>
<p>Notably in comparison to Maven, I have the language and scope folders swapped around. This simply keeps things closer together.</p>
<p>Now, hwo to actually make this work? Everything is done in Grunt, which makes life easier. I’m only going to concentrate on the src/node stack for now - the src/js stack is different because it builds a single output file using Webpack instead of building lots of files using Babel. I’ll cover how that works in a later post.</p>
<p>Firstly, because we have some explicit areas that we work in, I always set up some variables at the top of my Gruntfile to handle this:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> path = <span class="built_in">require</span>(<span class="string">"path"</span>);</div><div class="line"></div><div class="line"><span class="keyword">var</span> srcDir = <span class="string">"src"</span>;</div><div class="line"><span class="keyword">var</span> srcNodeDir = path.join(srcDir, <span class="string">"node"</span>);</div><div class="line"><span class="keyword">var</span> srcNodeMainDir = path.join(srcNodeDir, <span class="string">"main"</span>);</div><div class="line"><span class="keyword">var</span> srcNodeTestDir = path.join(srcNodeDir, <span class="string">"test"</span>);</div><div class="line"></div><div class="line"><span class="keyword">var</span> targetDir = <span class="string">"target"</span>;</div><div class="line"><span class="keyword">var</span> targetNodeDir = path.join(targetDir, <span class="string">"node"</span>);</div><div class="line"><span class="keyword">var</span> targetNodeMainDir = path.join(targetNodeDir, <span class="string">"main"</span>);</div><div class="line"><span class="keyword">var</span> targetNodeTestDir = path.join(targetNodeDir, <span class="string">"test"</span>);</div></pre></td></tr></table></figure></p>
<p>Next, because we’re creating a target area to work, it’s only polite to tidy it up as well. This is done with the grunt-contrib-clean module, configured as such:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">clean: &#123;</div><div class="line">    build: &#123;</div><div class="line">        src: targetDir</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Literally, this means that if you run “grunt clean” then it will delete the “target” directory. That’s it.</p>
<p>Next comes actual compilation of the source code from ES6 to ES5. This is done using Babel, and specifically with the grunt-babel module. Note that there are certain ES6 constructs that you may want to use that will also require the babel-runtime module being installed, and this as an actual dependency as opposed to a development dependency.</p>
<p>The configuration of Babel is a bit more involved, but not by much:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">babel: &#123;</div><div class="line">    options: &#123;</div><div class="line">        sourceMap: true,</div><div class="line">        optional: "runtime"</div><div class="line">    &#125;,</div><div class="line">    nodeMain: &#123;</div><div class="line">        files: [&#123;</div><div class="line">            expand: true,</div><div class="line">            cwd: srcNodeMainDir,</div><div class="line">            src: ["**/*.js"],</div><div class="line">            dest: targetNodeMainDir</div><div class="line">        &#125;]</div><div class="line">    &#125;,</div><div class="line">    test: &#123;</div><div class="line">        files: [&#123;</div><div class="line">            expand: true,</div><div class="line">            cwd: srcNodeTestDir,</div><div class="line">            src: ["**/*.js"],</div><div class="line">            dest: targetNodeTestDir</div><div class="line">        &#125;]</div><div class="line">    &#125;</div><div class="line">&#125;,</div></pre></td></tr></table></figure></p>
<p>Having done that, you can now run “grunt babel”, and you should see all of your ES6 files that you’ve written in src/node/main turn up as ES5 code in target/node/main, and likewise for src/node/test into target/node/test.</p>
<p>Unfortunately, it doesn’t quite work. Almost everything is fine, but module loading is a bit tricky. This is because of the module loader needing to know where to find the modules. The way that I get around this is a bit of a hack, but works, and that is to set the <em>NODE_PATH</em> environment variable. The way that I do this has the disadvantage that I can only easily run the code from Grunt, but that’s a small price to pay I find.</p>
<p>I do this by using the grunt-env and grunt-execute modules, and configuring as follows:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">env: &#123;</div><div class="line">    node: &#123;</div><div class="line">        NODE_PATH: targetNodeMainDir</div><div class="line">    &#125;</div><div class="line">&#125;,</div><div class="line">execute: &#123;</div><div class="line">    node: &#123;</div><div class="line">        src: path.join(targetNodeMainDir, "main.js")</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>I now need to change the task registrations to execute all of the plugins in the correct order. This now looks like:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">grunt.registerTask(<span class="string">"build"</span>, [<span class="string">"babel:node"</span>]);</div><div class="line">grunt.registerTask(<span class="string">"run"</span>, [<span class="string">"build"</span>, <span class="string">"env:node"</span>, <span class="string">"execute:node"</span>]);</div></pre></td></tr></table></figure></p>
<p>As of this point, executing “npm install” will correctly build all of your ES6 code into ES5 code, and executing “npm start” will do this and then execute the code you wrote in “src/node/main/main.js”. </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In the previous post, I set up a new NPM project to work with, and added in Grunt as a task runner so that we can do complex tasks if we want to. Next is setting it up to build ES6 code into ES5 via Babel, so that we get to use the newer features - Classes, Arrow Functions, Destructuring, Let/Const, and so on - whilst running on a runtime that doesn’t yet support them - because no runtime yet supports ES6 fully.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://babeljs.io/&quot;&gt;Babel&lt;/a&gt; is a Transpiler that we can use to automatically convert ES6 code into ES5 code for exactly this purpose. It literally takes existing ES6 source code as input, and converts it into ES5 source code as output - in the same way that a compiler would covert source code into machine code.&lt;/p&gt;
&lt;p&gt;If we so desired, we could use the exact same setup for working with &lt;a href=&quot;http://coffeescript.org/&quot;&gt;CoffeeScript&lt;/a&gt;, &lt;a href=&quot;https://www.dartlang.org/&quot;&gt;Dart&lt;/a&gt;, &lt;a href=&quot;http://www.typescriptlang.org/&quot;&gt;TypeScript&lt;/a&gt;, or any of a &lt;a href=&quot;https://github.com/jashkenas/coffeescript/wiki/List-of-languages-that-compile-to-JS&quot;&gt;growing set of languages&lt;/a&gt; that we can convert into Javascript.&lt;/p&gt;
    
    </summary>
    
      <category term="buildsystems" scheme="https://sazzer.github.io/blog/categories/buildsystems/"/>
    
    
      <category term="nodejs" scheme="https://sazzer.github.io/blog/tags/nodejs/"/>
    
      <category term="javascript" scheme="https://sazzer.github.io/blog/tags/javascript/"/>
    
      <category term="es6" scheme="https://sazzer.github.io/blog/tags/es6/"/>
    
      <category term="grunt" scheme="https://sazzer.github.io/blog/tags/grunt/"/>
    
  </entry>
  
  <entry>
    <title>Starting a NodeJS Project - Project Setup</title>
    <link href="https://sazzer.github.io/blog/2015/08/18/Starting-a-NodeJS-Project/"/>
    <id>https://sazzer.github.io/blog/2015/08/18/Starting-a-NodeJS-Project/</id>
    <published>2015-08-18T17:21:08.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>Whenever I start a new project using NodeJS, there are several things that I always do first. I thought that I’d finally get around to writing up what these things are, so that I can reference it myself, and in case anyone else might be interested. This covers part one of this writeup, setting up a base Node project, and adding Grunt in to it so that we can use Grunt as a task runner for more complicated builds. Next post will cover setting up the build so that we can write our code in ES6 instead, and later we will look at setting up some static analysis to keep code quality highter.</p>
<a id="more"></a>
<h3 id="Setting-up-package-json-correctly"><a href="#Setting-up-package-json-correctly" class="headerlink" title="Setting up package.json correctly"></a>Setting up package.json correctly</h3><p>Node and NPM use a file called “package.json” to configure your project. This file covers a lot of details, including smongst other things the proejct dependencies and actions to perform. This is relatively simple to set up, but it’s important to do it correctly. My bog standard template for this is simply:<br><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"name"</span>: <span class="string">"template-node"</span>,</div><div class="line">  <span class="attr">"version"</span>: <span class="string">"0.0.1"</span>,</div><div class="line">  <span class="attr">"description"</span>: <span class="string">""</span>,</div><div class="line">  <span class="attr">"scripts"</span>: &#123;</div><div class="line">  &#125;,</div><div class="line">  <span class="attr">"engines"</span>: &#123;</div><div class="line">    <span class="attr">"node"</span>: <span class="string">"&gt;=0.12"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="attr">"repository"</span>: &#123;</div><div class="line">    <span class="attr">"type"</span>: <span class="string">"git"</span>,</div><div class="line">    <span class="attr">"url"</span>: <span class="string">"https://github.com/sazzer/template-node.git"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="attr">"author"</span>: <span class="string">"Graham Cox &lt;graham@grahamcox.co.uk&gt;"</span>,</div><div class="line">  <span class="attr">"bugs"</span>: &#123;</div><div class="line">    <span class="attr">"url"</span>: <span class="string">"https://github.com/sazzer/template-node/issues"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="attr">"dependencies"</span>: &#123;</div><div class="line">  &#125;,</div><div class="line">  <span class="attr">"devDependencies"</span>: &#123;</div><div class="line">  </div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>This sets up the project with:</p>
<ul>
<li>A correct name, description and version number</li>
<li>A minimum requirement for the NodeJS version to use</li>
<li>Links to the Source Repository for the project</li>
<li>Links to the bug tracker for the project</li>
<li>Details of the author</li>
</ul>
<p>At the very least, this works. It’s nothing fancy, but it works.</p>
<h3 id="Adding-Grunt-into-the-mix"><a href="#Adding-Grunt-into-the-mix" class="headerlink" title="Adding Grunt into the mix"></a>Adding Grunt into the mix</h3><p>NPM has support for running certain lifecycle actions, but not many of them and not in a complicated manner. If you want to do anything more complicated then the choices are normally either Grunt or Gulp. Personally I’m a fan of Grunt - it’s a system that you describe What to do, as opposed to How to do it - and so that’s what I always use.</p>
<h4 id="Installing-Grunt"><a href="#Installing-Grunt" class="headerlink" title="Installing Grunt"></a>Installing Grunt</h4><p>Adding Grunt is easy. You need a bunch of dependencies, a Gruntfile, and a few entries in your package.json file to tell NPM to run Grunt for some of it’s lifecycle scripts. At a minimum the dependencies that I always use are:</p>
<ul>
<li>grunt-cli</li>
<li>jit-grunt</li>
<li>time-grunt</li>
</ul>
<p>These are all Development Dependencies, so can be added by doing:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install --save-dev grunt-cli jit-grunt time-grunt</div></pre></td></tr></table></figure></p>
<p>jit-grunt is a plugin that will automatically load Grunt plugins based solely on them being configured, which just makes the configuration file a bit cleaner to read. time-grunt is a plugin that will tell you at the end how long each step took so that you can see where all of the time is going. I use that purely because I find it interesting, but if you find that the build is too slow then it lets you work out where to optimise it.</p>
<h4 id="Configuring-Grunt"><a href="#Configuring-Grunt" class="headerlink" title="Configuring Grunt"></a>Configuring Grunt</h4><p>Grunt is configured by providing a Javascript file called “GruntFile.js”. The main goal of this file is to specify the configuration of all of the plugins that you want to use, and to set up the tasks that you want to run. </p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">module</span>.exports = <span class="function"><span class="keyword">function</span>(<span class="params">grunt</span>) </span>&#123;</div><div class="line">    <span class="built_in">require</span>(<span class="string">'jit-grunt'</span>)(grunt, &#123;</div><div class="line">    &#125;);</div><div class="line">    <span class="built_in">require</span>(<span class="string">'time-grunt'</span>)(grunt);</div><div class="line"></div><div class="line">    grunt.initConfig(&#123;</div><div class="line">        pkg: grunt.file.readJSON(<span class="string">'package.json'</span>)</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    grunt.registerTask(<span class="string">'build'</span>, []);</div><div class="line">    grunt.registerTask(<span class="string">'run'</span>, [<span class="string">'build'</span>]);</div><div class="line">    grunt.registerTask(<span class="string">'test'</span>, [<span class="string">'build'</span>]);</div><div class="line"></div><div class="line">    grunt.registerTask(<span class="string">'default'</span>, [<span class="string">'test'</span>]);</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>This sets up no plugins - because we don’t have any yet - and set up four tasks. The “default” task is the one that’s run when you don’t specify a task to run, and the other three are going to be called from NPM in a second.</p>
<p>Assuming that you’ve got the Grunt command-line tool installed globally - if not then do so now by running:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install -G grunt-cli</div></pre></td></tr></table></figure></p>
<p>then you can test these out by running the tasks from the command line:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ grunt <span class="built_in">test</span></div><div class="line"></div><div class="line">Done, without errors.</div><div class="line"></div><div class="line"></div><div class="line">Execution Time (2015-08-18 21:08:57 UTC)</div><div class="line">loading tasks  109ms  ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 100%</div><div class="line">Total 109ms</div></pre></td></tr></table></figure></p>
<h4 id="Calling-Grunt-from-NPM"><a href="#Calling-Grunt-from-NPM" class="headerlink" title="Calling Grunt from NPM"></a>Calling Grunt from NPM</h4><p>Once we’re happy that Grunt is set up and working correctly, we can add the configuration to NPM to run the tasks automatically. I do this by adding the following into the package.json file:<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">"scripts": &#123;</div><div class="line">  "install": "grunt",</div><div class="line">  "test": "grunt test",</div><div class="line">  "start": "grunt run"</div><div class="line">&#125;,</div></pre></td></tr></table></figure></p>
<p>This means that the default Grunt task will run when you do an “npm install”, and separately to that you can run “npm test” or “npm start” to run “grunt test” and “grunt run” respectively”</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Whenever I start a new project using NodeJS, there are several things that I always do first. I thought that I’d finally get around to writing up what these things are, so that I can reference it myself, and in case anyone else might be interested. This covers part one of this writeup, setting up a base Node project, and adding Grunt in to it so that we can use Grunt as a task runner for more complicated builds. Next post will cover setting up the build so that we can write our code in ES6 instead, and later we will look at setting up some static analysis to keep code quality highter.&lt;/p&gt;
    
    </summary>
    
      <category term="buildsystems" scheme="https://sazzer.github.io/blog/categories/buildsystems/"/>
    
    
      <category term="nodejs" scheme="https://sazzer.github.io/blog/tags/nodejs/"/>
    
      <category term="javascript" scheme="https://sazzer.github.io/blog/tags/javascript/"/>
    
      <category term="grunt" scheme="https://sazzer.github.io/blog/tags/grunt/"/>
    
  </entry>
  
  <entry>
    <title>Optimistic Locking in MongoDB</title>
    <link href="https://sazzer.github.io/blog/2015/07/18/Optimistic-Locking-in-MongoDB/"/>
    <id>https://sazzer.github.io/blog/2015/07/18/Optimistic-Locking-in-MongoDB/</id>
    <published>2015-07-18T09:31:31.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>MongoDB has a lot of good things about it, and equally a lot of bad things about it. This is true of most database engines though, and you need to know what you’re looking at to be able to make a good call over which system to use for what you’re doing.</p>
<p>Optimistic locking is traditionally quite difficult to achieve. You need to ensure that the version in the database matches the version in the update request, and fail if that’s not the case. There are a few ways of achieving this, but often with risks and race conditions involved.</p>
<a id="more"></a>
<p>MongoDB has the ability to execute a number of different atomic updates in a single statement, direct in the database. This can be used to our advantage here. Specifically here, we will be making use of the <a href="http://docs.mongodb.org/manual/reference/operator/update/set/" target="_blank" rel="external">$set</a>, <a href="http://docs.mongodb.org/manual/reference/operator/update/setOnInsert" target="_blank" rel="external">$setOnInsert</a> and <a href="http://docs.mongodb.org/manual/reference/operator/update/inc" target="_blank" rel="external">$inc</a> operators for this.</p>
<h3 id="Check-the-version-matches-first"><a href="#Check-the-version-matches-first" class="headerlink" title="Check the version matches first"></a>Check the version matches first</h3><p>The first thing would be to ensure that the database contains the same version as the update request. This could be done by querying the database first,  as follows:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.find(&#123;&quot;_id&quot;: &quot;abcdef&quot;&#125;)</div><div class="line">&#123; &quot;_id&quot; : &quot;abcdef&quot;, &quot;name&quot; : &quot;Test&quot;, &quot;description&quot; : &quot;Test Document&quot;, &quot;version&quot; : 1 &#125;</div><div class="line">&gt; db.test.save(&#123;&quot;_id&quot;: &quot;abcdef&quot;, &quot;version&quot;: 2, &quot;name&quot;: &quot;Test&quot;, &quot;description&quot;: &quot;Updated&quot;&#125;);</div><div class="line">WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)</div></pre></td></tr></table></figure>
<p>If this returns anything then we can check that the database version matches. This works well enough, except for the risk of somebody else introducing a change to the data between us doing this check and us doing the update. We’ll cover that in a bit</p>
<h3 id="Perform-atomic-updates-instead-of-a-complete-rewrite"><a href="#Perform-atomic-updates-instead-of-a-complete-rewrite" class="headerlink" title="Perform atomic updates instead of a complete rewrite"></a>Perform atomic updates instead of a complete rewrite</h3><p>The above replaced the entire document with a new version of it. This works, but requries that the entire document is provided every time. Also, and more importantly here, it requires that all of the new fields are computed by the client code and not the database. We can do better than that, using the atomic updates that are supported. Instead of the above, we can do the following:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.update(&#123;&quot;_id&quot;: &quot;abcdef&quot;&#125;, &#123;&quot;$set&quot;: &#123;&quot;name&quot;: &quot;Test&quot;, &quot;description&quot;: &quot;Updated&quot;, &quot;version&quot;: 3&#125;&#125;);</div><div class="line">WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)</div></pre></td></tr></table></figure></p>
<p>The two above statements do exactly the same. However, the update statement only sets the three fields specified - name, description and version - and doesn’t touch any other fields at all. </p>
<h3 id="Update-the-database-only-if-the-version-matches"><a href="#Update-the-database-only-if-the-version-matches" class="headerlink" title="Update the database only if the version matches"></a>Update the database only if the version matches</h3><p>You’ll notice that the above update statement starts with a query. We can use this to only update the database record if the version is the same. This means that we can avoid the check and race condition that goes along with it. Instead we ask the database to atomically update the record only if the version matches. This is simply done as follows:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.update(&#123;&quot;_id&quot;: &quot;abcdef&quot;, &quot;version&quot;: 3&#125;, &#123;&quot;$set&quot;: &#123;&quot;name&quot;: &quot;Test&quot;, &quot;description&quot;: &quot;Updated&quot;, &quot;version&quot;: 4&#125;&#125;);</div><div class="line">WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)</div></pre></td></tr></table></figure></p>
<p>If, however, we do this and the version doesn’t match then nothing gets updated:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.update(&#123;&quot;_id&quot;: &quot;abcdef&quot;, &quot;version&quot;: 3&#125;, &#123;&quot;$set&quot;: &#123;&quot;name&quot;: &quot;Test&quot;, &quot;description&quot;: &quot;Updated&quot;, &quot;version&quot;: 4&#125;&#125;);</div><div class="line">WriteResult(&#123; &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 0 &#125;)</div></pre></td></tr></table></figure></p>
<h3 id="Update-the-version-number-in-the-database"><a href="#Update-the-version-number-in-the-database" class="headerlink" title="Update the version number in the database"></a>Update the version number in the database</h3><p>Even with the above, we still need to know what the version number should be updated to before we can save it. This is not too bad in the case of the version number, but still if we can do it in the database then there are other benefits that we’ll see later.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.update(&#123;&quot;_id&quot;: &quot;abcdef&quot;, &quot;version&quot;: 4&#125;, &#123;&quot;$set&quot;: &#123;&quot;name&quot;: &quot;Test&quot;, &quot;description&quot;: &quot;Updated&quot;&#125;, &quot;$inc&quot;: &#123;&quot;version&quot;: 1&#125;&#125;);</div><div class="line">WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)</div></pre></td></tr></table></figure></p>
<p>In this case, we’re incrementing the version number by 1. This can be whatever rate we want to use, but importantly as long as we use the same for every update then that part of the document never changes. We dynamically write the version number in the query but not in the update. This makes our client code that little bit simpler.</p>
<h3 id="But-what-if-we’re-saving-a-new-document"><a href="#But-what-if-we’re-saving-a-new-document" class="headerlink" title="But what if we’re saving a new document?"></a>But what if we’re saving a new document?</h3><p>Now, all this is well and good, but we can go better still. MongoDB supports what’s called Upserting. This means that if the document to update doesn’t exist then we can instruct it to create a new one in its place. This is where the incrementing of the version number really wins.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.update(&#123;&quot;_id&quot;: &quot;fedcba&quot;&#125;, &#123;&quot;$set&quot;: &#123;&quot;name&quot;: &quot;Test&quot;, &quot;description&quot;: &quot;Updated&quot;&#125;, &quot;$inc&quot;: &#123;&quot;version&quot;: 1&#125;&#125;, &#123;&quot;upsert&quot;: true&#125;);</div><div class="line">WriteResult(&#123; &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 1, &quot;nModified&quot; : 0, &quot;_id&quot; : &quot;fedcba&quot; &#125;)</div></pre></td></tr></table></figure>
<p>You’ll notice that there are only two differences between this command and the previous one. We added a third parameter telling MongoDB to perform an Upsert, and we didn’t include a Version number in the query. Unfortunately, if you do include a version number - even with a value of null - then the “$inc” operator will fail on creating a new record. However, missing off that field altogether isn’t exactly hard. But what has it actually done?<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.find(&#123;&quot;_id&quot;: &quot;fedcba&quot;&#125;);</div><div class="line">&#123; &quot;_id&quot; : &quot;fedcba&quot;, &quot;name&quot; : &quot;Test&quot;, &quot;description&quot; : &quot;Updated&quot;, &quot;version&quot; : 1 &#125;</div></pre></td></tr></table></figure></p>
<p>It’s created a brand new document, with the ID that we queried for and a version number of 1. That’s pretty much perfect. You will notice that it used the ID that we queried for. This means that the client code must be responsible for ID generation. That’s not a huge problem though - just generate a UUID and you’re done. If you miss the ID out of the query altogether then you’ll instead update every single document in the database. </p>
<p>If you want to continue using the traditional Save command to save a new document and have MongoDB generate an ID for you then that’s also easy, but it does mean that Save and Create have different code paths. The above means that we can have a single code path for both. That’s a huge win.</p>
<h3 id="How-does-this-affect-optimistic-locking"><a href="#How-does-this-affect-optimistic-locking" class="headerlink" title="How does this affect optimistic locking?"></a>How does this affect optimistic locking?</h3><p>Now, given that Upserting means that we create a new document if the query doesn’t find an existing one, how does this affect Optimistic Locking? If we provide an ID and Version that don’t exist in the database, won’t that just create a new document instead and cause problems? No. MongoDB handles this by the fact that the ID must be unique. Again, this only works if you are generating IDs yourself.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.update(&#123;&quot;_id&quot;: &quot;fedcba&quot;, &quot;version&quot;: 1&#125;, &#123;&quot;$set&quot;: &#123;&quot;name&quot;: &quot;Test&quot;, &quot;description&quot;: &quot;Updated&quot;&#125;, &quot;$inc&quot;: &#123;&quot;version&quot;: 1&#125;&#125;, &#123;&quot;upsert&quot;: true&#125;);</div><div class="line">WriteResult(&#123;</div><div class="line">        &quot;nMatched&quot; : 0,</div><div class="line">        &quot;nUpserted&quot; : 0,</div><div class="line">        &quot;nModified&quot; : 0,</div><div class="line">        &quot;writeError&quot; : &#123;</div><div class="line">                &quot;code&quot; : 11000,</div><div class="line">                &quot;errmsg&quot; : &quot;E11000 duplicate key error index: test.test.$_id_ dup key: &#123; : \&quot;fedcba\&quot; &#125;&quot;</div><div class="line">        &#125;</div><div class="line">&#125;)</div></pre></td></tr></table></figure>
<p>Now, this time the error was different. It did indeed try to create a new record, but it failed because the key was already in use. </p>
<p>There is a problem with this, but it’s a very minor one. It does highlight how important the key generation is though. If you try to save a record with no version number - because it’s a new record - but with an ID that matches an existing one then you will update that existing one instead of creating a new one. This is potentially disasterous, but if your ID generation is unique enough - UUID has such a minimal chance of collisions that it’s not worth worrying about - then you can manage. If that’s not good enough then you may need to do something stricter - I can think of a few ID generation techniques that would work - or else use the traditional MongoDB mechanisms that will generate an ID for you on saving a new record.</p>
<h3 id="What-about-a-Creation-Date-field"><a href="#What-about-a-Creation-Date-field" class="headerlink" title="What about a Creation Date field?"></a>What about a Creation Date field?</h3><p>Sometimes you want to store on your document some details that are set when the document is created but are not updated. For example, a Creation Date or an Owner ID. The above would seem to make this problematic, since creation and editing of the document are the same command now. Once again, Mongo makes this easy for us to solve, using the “$setOnInsert” command. This is identical to the “$set” command, but only if the document was being created anew. If it already existed then it gets ignored. As such, we can do the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.update(&#123;&quot;_id&quot;: &quot;123456&quot;&#125;, &#123;&quot;$set&quot;: &#123;&quot;name&quot;: &quot;Test&quot;, &quot;description&quot;: &quot;Updated&quot;, &quot;modified&quot;: new Date()&#125;, &quot;$setOnInsert&quot;: &#123;&quot;created&quot;: new Date()&#125;, &quot;$inc&quot;: &#123;&quot;version&quot;: 1&#125;&#125;, &#123;&quot;upsert&quot;: true&#125;);</div><div class="line">WriteResult(&#123; &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 1, &quot;nModified&quot; : 0, &quot;_id&quot; : &quot;123456&quot; &#125;)</div><div class="line">&gt; db.test.find(&#123;&quot;_id&quot;: &quot;123456&quot;&#125;);</div><div class="line">&#123; &quot;_id&quot; : &quot;123456&quot;, &quot;name&quot; : &quot;Test&quot;, &quot;description&quot; : &quot;Updated&quot;, &quot;modified&quot; : ISODate(&quot;2015-07-18T11:16:55.818Z&quot;), &quot;created&quot; : ISODate(&quot;2015-07-18T11:16:55.818Z&quot;), &quot;version&quot; : 1 &#125;</div></pre></td></tr></table></figure>
<p>Which will create a brand new document, setting the modified and created dates to the current date and time. If we then do the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; db.test.update(&#123;&quot;_id&quot;: &quot;123456&quot;, &quot;version&quot;: 1&#125;, &#123;&quot;$set&quot;: &#123;&quot;name&quot;: &quot;Test&quot;, &quot;description&quot;: &quot;Updated&quot;, &quot;modified&quot;: new Date()&#125;, &quot;$setOnInsert&quot;: &#123;&quot;created&quot;: new Date()&#125;, &quot;$inc&quot;: &#123;&quot;version&quot;: 1&#125;&#125;, &#123;&quot;upsert&quot;: true&#125;);</div><div class="line">WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)</div><div class="line">&gt; db.test.find(&#123;&quot;_id&quot;: &quot;123456&quot;&#125;);</div><div class="line">&#123; &quot;_id&quot; : &quot;123456&quot;, &quot;name&quot; : &quot;Test&quot;, &quot;description&quot; : &quot;Updated&quot;, &quot;modified&quot; : ISODate(&quot;2015-07-18T11:17:15.732Z&quot;), &quot;created&quot; : ISODate(&quot;2015-07-18T11:16:55.818Z&quot;), &quot;version&quot; : 2 &#125;</div></pre></td></tr></table></figure>
<p>You’ll notice that the modiified date has changed but that the created date hasn’t. Again, one command for Create and Update and it just does The Right Thing.</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>All in all, MongoDB makes it possible to harmonise Create and Edit of records in a single statement, with virtually no risk of it doing the wrong thing. It may not be the right way to handle every data set, but I suspect it will work well enough for most.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MongoDB has a lot of good things about it, and equally a lot of bad things about it. This is true of most database engines though, and you need to know what you’re looking at to be able to make a good call over which system to use for what you’re doing.&lt;/p&gt;
&lt;p&gt;Optimistic locking is traditionally quite difficult to achieve. You need to ensure that the version in the database matches the version in the update request, and fail if that’s not the case. There are a few ways of achieving this, but often with risks and race conditions involved.&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="mongodb" scheme="https://sazzer.github.io/blog/tags/mongodb/"/>
    
  </entry>
  
  <entry>
    <title>Deploying your Maven Site to S3 using Travis</title>
    <link href="https://sazzer.github.io/blog/2015/06/13/Deploying-your-Maven-Site-to-S3-using-Travis/"/>
    <id>https://sazzer.github.io/blog/2015/06/13/Deploying-your-Maven-Site-to-S3-using-Travis/</id>
    <published>2015-06-13T10:16:51.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>Assuming you’ve got a Maven Build, and you’ve got a correctly configured Site for your build, and then further assuming that you are using the <a href="https://travis-ci.org/" target="_blank" rel="external">Travis CI</a> system for your Continuous Integration (It’s free, so why not), one potential next step is to automatically deploy your Maven Site somewhere.</p>
<a id="more"></a>
<p>This writeup shows how to publish your site to S3, and configure S3 to make it publicly accessable. There are plenty of other options out there, but S3 works well enough and turns out to be trivial to set up. All you need to make this work is:</p>
<ul>
<li>An AWS S3 Account</li>
<li>An S3 Bucket already set up</li>
<li>An AWS Access Key and Secret that can write to the bucket.</li>
</ul>
<p>If we’ve got those things then we’re good to go.</p>
<p>Firstly, we need to configure the maven build to be able to deploy to S3. I’ve done this using the <a href="https://github.com/jcaddel/maven-s3-wagon" target="_blank" rel="external">maven-s3-wagon</a> because it just works. The configuration that you need is twofold.</p>
<p>In the <em>build</em> section of your <em>pom.xml</em> file, you need to add the following. This makes it possible to use the S3 Wagon to upload your site.<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">extensions</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">extension</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.kuali.maven.wagons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-s3-wagon<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">extension</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">extensions</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>Then, in your <em>distributionManagement</em> section of your <em>pom.xml</em> file, you need to add something like the following:<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">distributionManagement</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">site</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>s3.site<span class="tag">&lt;/<span class="name">id</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>s3://<span class="tag">&lt;<span class="name">S3</span> <span class="attr">Bucket</span> <span class="attr">Name</span>&gt;</span>/<span class="tag">&lt;<span class="name">Site</span> <span class="attr">Path</span>&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">site</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">distributionManagement</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>The S3 Bucket Name is literally the name of the bucket that you’ve configured in S3 - for example, I’ve used “grahamcox.co.uk” for mine. The Site Path is some directory path under this bucket that you want to deploy the site - for example, you might use something like “${project.artifactId}/${project.version}/site” to keep all of your sites under a directory that is both Artifact and Version specific. This has benefits that when you release later versions of your site, you get to keep all of the old sites, including all of your javadoc and everything, so that people can refer back to it. The path that you deploy under is completely up to you of course.</p>
<p>Once you’ve updated the pom file as above, you are now ready to actually deploy the site. You can test this by running <em>mvn site:deploy</em> by hand - and if you do this it will likely fail due to not having any authentication credentials available. This is the next step.</p>
<p>The Maven Plugin has documentation on how to configure authentication for S3 available at <a href="https://github.com/jcaddel/maven-s3-wagon/wiki/Authentication" target="_blank" rel="external">https://github.com/jcaddel/maven-s3-wagon/wiki/Authentication</a>. This is work a read just to make sure you understand what’s happening, and you can also set it up locally to do a test deploy to make sure you’re happy with the deployment.</p>
<p>The way that I configured Travis to do this is using the Environment Variables, and specifically I used encrypted ones so that my AWS Key and Secret aren’t just available on the internet in plaintext for anyone to see.</p>
<p>To do this, you first need to install the <a href="https://github.com/travis-ci/travis.rb" target="_blank" rel="external">Travis CLI tools</a>. These are well documented on the Github page at <a href="https://github.com/travis-ci/travis.rb#installation" target="_blank" rel="external">https://github.com/travis-ci/travis.rb#installation</a>, but in short you do:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ sudo gem install travis</div><div class="line">.....</div><div class="line">$ travis login</div><div class="line">.....</div><div class="line">$</div></pre></td></tr></table></figure></p>
<p>And then you’re good to go. Once these are available, you can configure Travis to depoy your site.</p>
<p>First, tell Travis about your AWS Keys. This is done as follows:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ travis encrypt --add env.glocal AWS_ACCESS_KEY_ID=abcdef AWS_SECRET_KEY=123456</div></pre></td></tr></table></figure></p>
<p>Where the Access Key is “abcdef” and the Secret Key is “123456”. (These are made up. Don’t try them. They won’t work).</p>
<p>This adds a new Encrypted setting to the Travis Build to make the Environment Variables AWS_ACCESS_KEY_ID and AWS_SECRET_KEY available throughout the build.</p>
<p>Secondly, tell travis to do a site deploy. I did this as follows, in my .travis.yml file<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="attr">language:</span> java</div><div class="line"><span class="attr">jdk:</span></div><div class="line"><span class="bullet">    -</span> oraclejdk8</div><div class="line"><span class="attr">install:</span> mvn clean install -P-quality-checks -DskipTests</div><div class="line"><span class="attr">script:</span></div><div class="line"><span class="bullet">    -</span> mvn install</div><div class="line"><span class="attr">after_script:</span> mvn site site:deploy</div></pre></td></tr></table></figure></p>
<p>Note that my build does not run tests or quality checks (PMD, FindBugs, Checkstyle, etc) as part of the Install phase, but does that all later on in the Script phase. I then do the site deploy after the script phase has run, whether the build was successful or not. This means that if I break the build, my site still gets deployed and tells me what my errors are. (In theory. I’ve not tested that yet)</p>
<p>Once all of this is done, you can now visit the Site deployed in S3 and marvel at it’s beauty.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Assuming you’ve got a Maven Build, and you’ve got a correctly configured Site for your build, and then further assuming that you are using the &lt;a href=&quot;https://travis-ci.org/&quot;&gt;Travis CI&lt;/a&gt; system for your Continuous Integration (It’s free, so why not), one potential next step is to automatically deploy your Maven Site somewhere.&lt;/p&gt;
    
    </summary>
    
      <category term="buildsystems" scheme="https://sazzer.github.io/blog/categories/buildsystems/"/>
    
    
      <category term="travisci" scheme="https://sazzer.github.io/blog/tags/travisci/"/>
    
      <category term="s3" scheme="https://sazzer.github.io/blog/tags/s3/"/>
    
  </entry>
  
  <entry>
    <title>I really try to like Node.js (Part 1)</title>
    <link href="https://sazzer.github.io/blog/2015/05/16/I-really-try-to-like-Node-js/"/>
    <id>https://sazzer.github.io/blog/2015/05/16/I-really-try-to-like-Node-js/</id>
    <published>2015-05-16T09:45:14.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>I like the idea behind using Node.js for webapp development. I really do. The idea of using a consistent language between both client and server, which therefore means that you are able to share code between the two without replication. Who wouldn’t like that?</p>
<a id="more"></a>
<p>The problem is, I’m spoilt. I’m a Java Enterprise developer by trade, so I come into personal projects with a lot of expectations, simply because it’s what I would want to do at work. I try my best to relax these, but sometimes it’s just going too far. Specifically, a subset of the expectations that I want when doing my projects would be:</p>
<ul>
<li>Good test support<ul>
<li>Including Unit tests, Integration tests and Verification tests</li>
</ul>
</li>
<li>Code coverage</li>
<li>Static analysis<ul>
<li>To an extent, this includes compile time checks</li>
</ul>
</li>
<li>Documentation support</li>
</ul>
<p>Now, a lot of this you can get in Node. Of course, me being me I then further complicate things and make it hard to get some of these. (I’ve taken a real liking to ES6 instead of ES5, but because ES6 is best supported by Transpiling with something like <a href="https://babeljs.io/" target="_blank" rel="external">Babel</a> or <a href="https://github.com/google/traceur-compiler" target="_blank" rel="external">Traceur</a>, it means that the code that you run isn’t the code that you write. That in turn makes code coverage essentially pointless.)</p>
<p>Documentation of your code in Node is easy enough. There’s a fair few projects that will cover this for you, including the one that I’ve been using - <a href="https://esdoc.org/" target="_blank" rel="external">esdoc</a> - that supports ES6 constructs. To be fair, any language that didn’t allow you to document your code would be a real non-starter.</p>
<p>Static analysis of Javascript has a few different alternatives. On the one hand, with ES5 code you have tools like <a href="http://jshint.com/" target="_blank" rel="external">jshint</a>. On the other hand, because I’ve been working with ES6 code then the transpiler does a level of this automatically so that’s not a problem.</p>
<p>Code coverage is possible in Node, but as I say when working with ES6 code it’s relatively pointless. You want to know how much of the code that you wrote was tested, not how much of the code that was generated by the transpiler. However, for people who want to do this there are tools like <a href="http://blanketjs.org/" target="_blank" rel="external">blanket</a> and <a href="https://github.com/gotwarlost/istanbul" target="_blank" rel="external">instanbul</a>.</p>
<p>And of course, Node has a large number of libraries for testing your code. I’ve been a fan of <a href="http://mochajs.org/" target="_blank" rel="external">Mocha</a> + <a href="http://chaijs.com/" target="_blank" rel="external">Chai</a>, but there are plenty of others to choose from as well, both for the actual test harness and for the assertions. That’s not the problem at all.</p>
<p>This is where my problems start. When you write Unit Tests - as opposed to Integration Tests - then the idea is that you test one single Unit in isolation. This typically means that whenever your unit has external dependencies then you replace them with something that is under your control. This something is a Mock or a Stub. Now, there are ways of doing this in Node. In particular, I’ve been using <a href="http://sinonjs.org/" target="_blank" rel="external">Sinon</a> as a Mocking library because it actually gives very good support for creating mocks and stubs, as well as what it calls spies, and lets you confirm that you module works as expected.</p>
<p>The rub is getting these mocks into your code. The ES6 module system encourages you to have modules that include public (i.e. exported) and private components, and that depend directly on the public parts of other modules. There’s no need for any kind of Dependency Injection system with all of this, because you just depend on the modules in question and tie things together that way. And for writing the code itself, that works really well. The problem is that for testing your code this is a complete nightmare. Working this way, you would need some way of overriding the dependencies that are pulled in to your modules after they have loaded. Now, there are modules for traditional ES5 Node that allow you to do this - e.g. <a href="https://github.com/jhnns/rewire" target="_blank" rel="external">Rewire</a> - but they all feel a bit hacky, and they fall down quite badly when you start trying to do ES6 node instead. Rewire requires that you use it instead of the built in ‘require’ statement, which in ES6 you don’t use (At least not directly. Babel re-writes the import statements to be require statements for you). <a href="https://github.com/mfncooper/mockery" target="_blank" rel="external">Mockery</a>, on the other hand, breaks in on the Node module loading system, but you need to have set it up <em>before</em> the appropriate require calls are made, which you can’t do in ES6 since the import statements must be top level.</p>
<p>And so, you end up with lots of units of code that you just don’t bother to test, because you can’t be bothered to work out how to get around this problem. And that in turn means that your code is a bit brittle to change, because you don’t have the coverage necessary to know that things still work when you change it. And then that leads to code that you’re scared of changing, which means that it ends up just as a mess. My latest project is only 17 files large - I’ve barely started - and it’s already feeling messy and out of control because of this.</p>
<p>And that sucks, because on the face of it I like Node.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I like the idea behind using Node.js for webapp development. I really do. The idea of using a consistent language between both client and server, which therefore means that you are able to share code between the two without replication. Who wouldn’t like that?&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="nodejs" scheme="https://sazzer.github.io/blog/tags/nodejs/"/>
    
      <category term="rant" scheme="https://sazzer.github.io/blog/tags/rant/"/>
    
  </entry>
  
  <entry>
    <title>Javascript modules - ES5 vs ES6</title>
    <link href="https://sazzer.github.io/blog/2015/05/12/Javascript-modules-ES5-vs-ES6/"/>
    <id>https://sazzer.github.io/blog/2015/05/12/Javascript-modules-ES5-vs-ES6/</id>
    <published>2015-05-12T20:44:29.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>The latest version of the ECMAScript Language - <a href="http://wiki.ecmascript.org/doku.php?id=harmony:specification_drafts" target="_blank" rel="external">ES6</a> - introduces a lot of new features. One of the most interesting of these is the introduction of a module system that is built into the language. The way this works is, unfortunately, very different to how any of the pre-existing ES5 module systems work.</p>
<p>Wait, what? Most of the time when people discuss the new ES6 module system it is talked about as being one of the brand new features of ES6, and not an existing feature that has been fit into the language - such as Promises. However, there are already a number of module systems that are used and very well supported in ES5. The fact that there are different module systems, and that they aren’t trivially compatible, is a big problem. There are ways of making the three major module systems work together, but it’s not great to have to do that just to work around this fact. As such, the fact that there is a language-level module system in ES6 is a good step forwards. It’s just that the new module system isn’t a perfect solution.</p>
<a id="more"></a>
<h3 id="Existing-ES5-Module-Systems"><a href="#Existing-ES5-Module-Systems" class="headerlink" title="Existing ES5 Module Systems"></a>Existing ES5 Module Systems</h3><p>One thing that is worth remembering here. Just because ES6 introduces a whole new module system as part of the language doesn’t mean that you can’t use the other module systems. If it works for you, then use it. It’s even possible to mix and match to an extent, though you need to be careful there for obvious reasons.</p>
<h4 id="AMD-Asynchronous-Module-Definition"><a href="#AMD-Asynchronous-Module-Definition" class="headerlink" title="AMD (Asynchronous Module Definition)"></a>AMD (Asynchronous Module Definition)</h4><p>AMD is the module system that works well for browser-based Javascript, because of how the modules get loaded. The most common implementation of this is <a href="http://requirejs.org/" target="_blank" rel="external">RequireJS</a>, and the code roughly looks like:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">define([<span class="string">'module-a'</span>, <span class="string">'module-b'</span>], <span class="function"><span class="keyword">function</span>(<span class="params">a, b</span>) </span>&#123;</div><div class="line">    <span class="comment">// Here, the variable 'a' is bound to module-a, and 'b' is bound to module-b</span></div><div class="line"></div><div class="line">    <span class="comment">// Here we define a brand new module that has three different fields, 'func', 'integer' and 'string'</span></div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">        func: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;...&#125;, </div><div class="line">        integer: a.integer,</div><div class="line">        string: b.string;</div><div class="line">    &#125;;</div><div class="line">&#125;);</div></pre></td></tr></table></figure></p>
<p>This works well in a browser because the define() function can register a module, and then the loader can asynchronously load all of the dependencies and then call the module only when everything is available.</p>
<p>In this case, a single file is at least one module, but can be more than that if you so desire. If you want you can specify a name for the module in the define() function, but if you don’t then the assumption is that a single file is a single module and the filename is used for the module name.</p>
<h4 id="CommonJS"><a href="#CommonJS" class="headerlink" title="CommonJS"></a>CommonJS</h4><p>CommonJS is the module system that works well for server-side Javascript, again because of how the loading works. The most common implementation of this is <a href="https://nodejs.org/" target="_blank" rel="external">Node.JS</a>, and the above code would look like:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> a = <span class="built_in">require</span>(<span class="string">'module-a'</span>);</div><div class="line"><span class="keyword">var</span> b = <span class="built_in">require</span>(<span class="string">'module-b'</span>);</div><div class="line"><span class="comment">// Here, the variable 'a' is bound to module-a, and 'b' is bound to module-b</span></div><div class="line"></div><div class="line"><span class="comment">// Here we define a brand new module that has three different fields, 'func', 'integer' and 'string'</span></div><div class="line"><span class="built_in">module</span>.exports = &#123;</div><div class="line">    func: <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;...&#125;, </div><div class="line">    integer: a.integer,</div><div class="line">    string: b.string;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></p>
<p>NodeJS works well here because the require() method is synchronous, so loading a module will pause the entire program whilst all of the dependant modules are loaded, and if a dependant module fails to load then an error occurs. </p>
<p>In this case, a single file is always a single module, and the module is identified by the file path.</p>
<p>It can be used in a browser by means of something like <a href="https://webpack.github.io/" target="_blank" rel="external">Webpack</a> if you really want, which then can combine all of your browser-side javascript into a single file for more efficient loading. </p>
<h4 id="Globals"><a href="#Globals" class="headerlink" title="Globals"></a>Globals</h4><p>Whilst technically not a module system per se, the third major alternative that gets used is simply to use global variables for everything and to just import everything by hand. It’s crap, but it works.</p>
<h3 id="ES6-Module-System"><a href="#ES6-Module-System" class="headerlink" title="ES6 Module System"></a>ES6 Module System</h3><p>In ES6, the module system that has been introduced works completely differently to the above. Both AMD and CommonJS work on the assumption that you will want all of a module, or none of it. ES6 starts out with a completely different assumption - that a module exports a number of entities, and that another module will want some subset of those entities - which may be all of them.</p>
<p>At it’s core, the ES6 module system has two complementary concepts - Exporting and Importing. A single file is always a single module, and a single module will export a number of different entities and import a number of other entities.</p>
<p>The above examples can be re-written as follows using ES6:<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Here we import the entity called 'integer' from 'module-a', and the entity called 'string' from 'module-b'. Nothing else is imported</span></div><div class="line"><span class="keyword">import</span> &#123;integer&#125; <span class="keyword">from</span> <span class="string">'module-a'</span>;</div><div class="line"><span class="keyword">import</span> &#123;string&#125; <span class="keyword">from</span> <span class="string">'module-b'</span>;</div><div class="line"></div><div class="line"><span class="comment">// Here we export three different fields, 'func', 'integer' and 'string'</span></div><div class="line"><span class="keyword">export</span> <span class="function"><span class="keyword">function</span> <span class="title">func</span>(<span class="params"></span>) </span>&#123;...&#125;</div><div class="line"><span class="keyword">export</span> integer;</div><div class="line"><span class="keyword">export</span> string;</div></pre></td></tr></table></figure></p>
<p>Firstly, note how different the above looks. The import and export keywords are brand new, and the import line isn’t even valid in ES5. Secondly, note that you can declare that you export an entity at the point you declare it, rather than needing to declare everything first and then export it all later on. Finally, the import mechanism is very specific as to what you import. You pull in a very specific entity from a module and you give it a very specific name.</p>
<p>The export mechanism is fantastic. You no longer need to repeat yourself when you define a module, and you no longer need to write some convoluted code to get things working as you want. You simply put the word “export” before the declaration and that entity will be exported. Simple.</p>
<p>The import mechanism also has a lot going for it. There are a lot of variations on how you can import entities, and you only need to import the very specific pieces of a module that you actually care about. I’m not going to go into the details of how it all works - there’s plenty of that already out there.</p>
<p>I do have some problems with all of this though. Specifically, I have the following problems:</p>
<ol>
<li>The import statement is non-trivial to read and to work out exactly what you want to do. There’s a lot of variation on how it works, and it can be difficult to get it exactly right. Odds are, if you are working with ES6 modules all the way then you will almost always use the above syntax, but not always. </li>
<li>You can’t import the entire module and assign that to a variable. In both AMD and CommonJS, the import mechanism works exactly by assigning the entire module to a variable in this module, and then you access the pieces that you care about. In ES6 that is not possible. At all. This means that it’s difficult to write one module that is a combination of other modules in a way that is non-repetative.</li>
<li>You can’t import modules dynamically. (That’s not quite true. There are ways in pure ES6, but not so easily in some of the transpilers). You can do this in CommonJS as a side effect of the above, and that makes certain things really easy to implement - such as loading all of the modules in a certain directory automatically without needing to specify them.</li>
</ol>
<p>Note that #3 above isn’t possible in AMD either, but it still feels like a step backwards in comparison to the CommonJS module system in some ways. I do understand why these restrictons are in place though - namely the fact that dynamic module loading is really hard to do in a remote environment.</p>
<p>In general, I like the new ES6 module system. I just think it’s a shame that it’s not compatible with any of the existing ones, but overall it’s a step in the right direction.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The latest version of the ECMAScript Language - &lt;a href=&quot;http://wiki.ecmascript.org/doku.php?id=harmony:specification_drafts&quot;&gt;ES6&lt;/a&gt; - introduces a lot of new features. One of the most interesting of these is the introduction of a module system that is built into the language. The way this works is, unfortunately, very different to how any of the pre-existing ES5 module systems work.&lt;/p&gt;
&lt;p&gt;Wait, what? Most of the time when people discuss the new ES6 module system it is talked about as being one of the brand new features of ES6, and not an existing feature that has been fit into the language - such as Promises. However, there are already a number of module systems that are used and very well supported in ES5. The fact that there are different module systems, and that they aren’t trivially compatible, is a big problem. There are ways of making the three major module systems work together, but it’s not great to have to do that just to work around this fact. As such, the fact that there is a language-level module system in ES6 is a good step forwards. It’s just that the new module system isn’t a perfect solution.&lt;/p&gt;
    
    </summary>
    
      <category term="programming" scheme="https://sazzer.github.io/blog/categories/programming/"/>
    
    
      <category term="nodejs" scheme="https://sazzer.github.io/blog/tags/nodejs/"/>
    
      <category term="javascript" scheme="https://sazzer.github.io/blog/tags/javascript/"/>
    
      <category term="es5" scheme="https://sazzer.github.io/blog/tags/es5/"/>
    
      <category term="es6" scheme="https://sazzer.github.io/blog/tags/es6/"/>
    
  </entry>
  
  <entry>
    <title>Deploying Hexo to Github Pages with Travis</title>
    <link href="https://sazzer.github.io/blog/2015/05/04/Deploying-Hexo-to-Github-Pages-with-Travis/"/>
    <id>https://sazzer.github.io/blog/2015/05/04/Deploying-Hexo-to-Github-Pages-with-Travis/</id>
    <published>2015-05-04T19:32:38.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>I’ve just recently been working on getting my Hexo blog set up, and importantly on getting it automatically deploy to Github Pages every time I do a commit. Given that this is a generated site, there are intermediate steps involved between the source that is committed and the site that is deployed. </p>
<p>The obvious way to achieve the actual Build part of this is to use the fantastic <a href="https://travis-ci.org/" target="_blank" rel="external">Travis CI</a>, which can be set up to perform a build every time you do a commit to a Github Repository. The challenge involved in this is getting Travis to be able to push the deployed site back to Github for it to be accessed. However, it turns out that Github have a solution that can work for this as well.</p>
<p>From this point on, I’m going to assume that you’ve already got Hexo configured as you want it, and you know how to configure Travis to build a Github repository correctly.</p>
<a id="more"></a>
<h3 id="Configure-Travis-to-build-Hexo"><a href="#Configure-Travis-to-build-Hexo" class="headerlink" title="Configure Travis to build Hexo"></a>Configure Travis to build Hexo</h3><p>The first thing that I did was to configure Travis to build the Hexo site, but not deploy it. This is simply a case of enabling the Travis hooks for this repository, and adding a .travis.yml file as follows:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="attr">language:</span> node_js</div><div class="line"><span class="attr">branches:</span></div><div class="line"><span class="attr">  only:</span></div><div class="line"><span class="bullet">  -</span> master</div><div class="line"><span class="attr">before_install:</span></div><div class="line"><span class="bullet">-</span> npm install -g hexo</div><div class="line"><span class="bullet">-</span> npm install</div><div class="line"><span class="attr">install:</span></div><div class="line"><span class="bullet">-</span> hexo generate</div></pre></td></tr></table></figure></p>
<p>This configures Travis to treat the repository as a Node.JS build, to only ever build when the “master” branch changes (This is important, otherwise we get an infinite loop caused by Travis updating the repo itself), and to actually install Hexo and run the generate step.</p>
<h3 id="Configure-Hexo-to-deploy-to-Git"><a href="#Configure-Hexo-to-deploy-to-Git" class="headerlink" title="Configure Hexo to deploy to Git"></a>Configure Hexo to deploy to Git</h3><p>Next we need to configure Hexo to deploy to Github correctly. This is best done using the existing “hexo-deployer-git” plugin, which does exactly what we need. Installing this is as simple as:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install --save hexo-deployer-git</div></pre></td></tr></table></figure>
<p>and then entering the following into the _config.yml file in the Hexo root. </p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="attr">deploy:</span></div><div class="line"><span class="attr">    type:</span> git</div><div class="line"><span class="attr">    repo:</span> git@github.com:sazzer/blog.git</div><div class="line"><span class="attr">    branch:</span> gh-pages</div></pre></td></tr></table></figure>
<p>The branch needs to be “gh-pages” to work with the Github Pages system, and the repo needs to be the repository that you wish to deploy to. Note that I’ve used the SSH version of the URL here, which means that you can deploy to it manually but Travis will not be able to, unless you’re daft enough to configure your Github account with an SSH Key that Travis uses. (Don’t do that)</p>
<h3 id="Set-up-a-Github-Token-to-use-for-deployments"><a href="#Set-up-a-Github-Token-to-use-for-deployments" class="headerlink" title="Set up a Github Token to use for deployments"></a>Set up a Github Token to use for deployments</h3><p>Travis is unable to push to your Github repository by default. This makes a lot of sense from a security point of view - if Travis can push to your repository then it can change your code without your permission. However, Github have an OAuth style system where you can use an alternative URL to push to that uses an OAuth token instead of an SSH key. </p>
<p>Firstly you need a Token. You can generate these by visiting the <a href="https://github.com/settings/tokens" target="_blank" rel="external">Personal access tokens</a> page in your Github settings. Simply configure a new token here, and make sure that it has the “repo” permissions at least.</p>
<p>When we want to use this token to actually push to Github, we use a different URL to the usual one. This isn’t instantly obvious, but the URL that you need to use is: https://<token>:x-oauth-basic@github.com/<user>/<repo>.git. So, for this blog the URL is https://<token>:x-oauth-basic@github.com/sazzer/blog.git.</token></repo></user></token></p>
<h3 id="Rewrite-the-Hexo-config-to-use-the-Github-Token"><a href="#Rewrite-the-Hexo-config-to-use-the-Github-Token" class="headerlink" title="Rewrite the Hexo config to use the Github Token"></a>Rewrite the Hexo config to use the Github Token</h3><p>We obviously don’t want to ever commit the Github Token to the repository in plain text - that would be silly. If we do that then anyone who can read the repo, which for Public repositories means everyone, can see the token and then push changes to it. What’s worse, the token is applied to the entire account, not just to a single repository so if someone compromises this token then they can write to <em>every</em> repository you’ve got.</p>
<p>Thankfully, Travis has a solution here too. Travis supports setting Encrypted settings in your .travis.yml file, so you can safely add an encrypted setting that stores your Github token safe in the knowledge that only Travis builds of this specific Github repository - not forks of it - can read it.</p>
<p>To do this you need to use the Travis command line tools. This is a simple Ruby gem, so you can install it with:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ gem install travis</div><div class="line">$ travis login</div></pre></td></tr></table></figure>
<p>Once installed you then add a new setting to your .travis.yml file with:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ travis encrypt <span class="string">'GH_TOKEN=&lt;TOKEN&gt;'</span> --add</div></pre></td></tr></table></figure></p>
<p>This will add a new section to your .travis.yml file along the lines of:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="attr">env:</span></div><div class="line"><span class="attr">  global:</span></div><div class="line"><span class="attr">      secure:</span> X7+<span class="number">8</span>XAko5oQgDTuPbLEQVlb0qwXjLkz1Pgia6eGL4Q/<span class="number">0</span>kZ89hGzGtOrKAhmtZRpcmg+PBxrLo5bdziAP/rvFslIabHHYBYv8ES8VrA81B/Q+t1VFbQEfhZdSq/L0wVsyBe2p6OOu2bNOLjPMG//aaynLxXstEEVSHh/lSuzsE4A=</div></pre></td></tr></table></figure></p>
<p>That long string is the encrypted environment settings that we want to use. When the Travis build executes then an environment variable called “GH_TOKEN” will exist with the Github token in it.</p>
<p>The next trick here is to dynamically re-write the Hexo configuration file to use the Token based URL instead of the SSH based one. This is done simply using sed, because why not. All I did here was to add the following to the .travis.yml file</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="attr">before_script:</span></div><div class="line"><span class="bullet">-</span> git config --global user.name <span class="string">'Graham Cox via Travis CI'</span></div><div class="line"><span class="bullet">-</span> git config --global user.email <span class="string">'graham@grahamcox.co.uk'</span></div><div class="line"><span class="bullet">-</span> sed -i<span class="string">''</span> <span class="string">"s~git@github.com:sazzer/blog.git~https://$&#123;GH_TOKEN&#125;:x-oauth-basic@github.com/sazzer/blog.git~"</span> _config.yml</div></pre></td></tr></table></figure>
<p>The trick here is that we take the repository string that is in the _config.yml file that represents the SSH repository URL, and replace it with the HTTP Token based version. We also set up a global username and email so that the commit has a suitable author.</p>
<h3 id="Actually-deploy-from-Travis"><a href="#Actually-deploy-from-Travis" class="headerlink" title="Actually deploy from Travis"></a>Actually deploy from Travis</h3><p>The only thing left to do is to get Travis to actually deploy the generated site to Github. This is a simple case of adding the following to the .travis.yml file<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">script:</span></div><div class="line"><span class="bullet">-</span> hexo deploy --silent</div></pre></td></tr></table></figure></p>
<p>The “–silent” is important here, because otherwise the Travis logs will include the full Push URL, which itself includes the Github Token that we must keep secret. </p>
<p>I’ve seen other writeups that use “after_script” instead, but if you use that then the deploy failing doesn’t cause the Travis build to break, so I’ve used “script” instead.</p>
<p>And that’s it. After all of the above, I can write a new post, do a push to Github and a few minutes later it’s live.</p>
<p>You can see the actual configuration that I’ve used in the following files:</p>
<ul>
<li><a href="https://github.com/sazzer/blog/blob/master/.travis.yml" target="_blank" rel="external">.travis.yml</a></li>
<li><a href="https://github.com/sazzer/blog/blob/master/_config.yml" target="_blank" rel="external">_config.yml</a></li>
<li><a href="https://github.com/sazzer/blog/blob/master/package.json" target="_blank" rel="external">package.json</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I’ve just recently been working on getting my Hexo blog set up, and importantly on getting it automatically deploy to Github Pages every time I do a commit. Given that this is a generated site, there are intermediate steps involved between the source that is committed and the site that is deployed. &lt;/p&gt;
&lt;p&gt;The obvious way to achieve the actual Build part of this is to use the fantastic &lt;a href=&quot;https://travis-ci.org/&quot;&gt;Travis CI&lt;/a&gt;, which can be set up to perform a build every time you do a commit to a Github Repository. The challenge involved in this is getting Travis to be able to push the deployed site back to Github for it to be accessed. However, it turns out that Github have a solution that can work for this as well.&lt;/p&gt;
&lt;p&gt;From this point on, I’m going to assume that you’ve already got Hexo configured as you want it, and you know how to configure Travis to build a Github repository correctly.&lt;/p&gt;
    
    </summary>
    
      <category term="blog" scheme="https://sazzer.github.io/blog/categories/blog/"/>
    
    
      <category term="hexo" scheme="https://sazzer.github.io/blog/tags/hexo/"/>
    
      <category term="github" scheme="https://sazzer.github.io/blog/tags/github/"/>
    
      <category term="travisci" scheme="https://sazzer.github.io/blog/tags/travisci/"/>
    
  </entry>
  
  <entry>
    <title>Hexo for Static Generated blogs</title>
    <link href="https://sazzer.github.io/blog/2015/05/04/Hexo-for-Static-Generated-blogs/"/>
    <id>https://sazzer.github.io/blog/2015/05/04/Hexo-for-Static-Generated-blogs/</id>
    <published>2015-05-04T14:41:24.000Z</published>
    <updated>2016-09-03T13:23:59.656Z</updated>
    
    <content type="html"><![CDATA[<p>Blogs are not a new thing. However, a re-emerging trend is to use Static Site Generators that take source code stored in a source control system, compile this up into static HTML files and deploy the compiled HTML as the site. There are obvious drawbacks to this style of doing things, but at the same time there are a fair few benefits to it as well - more so for certain type of people who are used to thinking in terms of code that gets compiled into the results (i.e. us developers).</p>
<p>Having decided to use a Static Site Generator, there’s then the question of which one to use. There are plenty to choose from, but the three big ones are:</p>
<ul>
<li><a href="http://jekyllrb.com/" target="_blank" rel="external">Jekyll</a></li>
<li><a href="http://octopress.org/" target="_blank" rel="external">Octopress</a></li>
<li><a href="https://hexo.io/" target="_blank" rel="external">Hexo</a></li>
</ul>
<p>Octopress is actually just a fork of Jekyll.  Both of these are written in Ruby, and have a relatively large number of plugins and themes to select from.  Hexo, on the other hand, is written in Node.JS and whilst it has plugins and themes, the selection isn’t quite as much.  However, from having played with both Hexo and Jekyll, the architecture of Hexo just feels better.  The big problem I had with Jekyll was how the themes were so tightly integrated into the actual configuation.  Changing themes in Jekyll is not a trivial thing to do. Because of this, I’ve chosen Hexo as my generator of choice.   </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Blogs are not a new thing. However, a re-emerging trend is to use Static Site Generators that take source code stored in a source control
    
    </summary>
    
      <category term="blog" scheme="https://sazzer.github.io/blog/categories/blog/"/>
    
    
      <category term="hexo" scheme="https://sazzer.github.io/blog/tags/hexo/"/>
    
  </entry>
  
</feed>
